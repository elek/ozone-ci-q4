2019-09-26 20:33:18,570 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-26 20:33:18,668 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-26 20:33:18,671 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-26 20:33:18,689 [main] INFO  util.log (Log.java:initialized(192)) - Logging initialized @942ms
2019-09-26 20:33:18,794 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: deletedBlocks
2019-09-26 20:33:18,794 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:deletedBlocks
2019-09-26 20:33:18,795 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: validCerts
2019-09-26 20:33:18,795 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:validCerts
2019-09-26 20:33:18,795 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: revokedCerts
2019-09-26 20:33:18,795 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:revokedCerts
2019-09-26 20:33:18,808 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: default
2019-09-26 20:33:18,808 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(167)) - Using default column profile:DBProfile.DISK for Table:default
2019-09-26 20:33:18,809 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:getDbProfile(198)) - Using default options. DBProfile.DISK
2019-09-26 20:33:19,057 [main] INFO  net.NodeSchemaLoader (NodeSchemaLoader.java:loadSchemaFromFile(125)) - Loading file from sun.misc.CompoundEnumeration@71d44a3
2019-09-26 20:33:19,059 [main] INFO  net.NodeSchemaLoader (NodeSchemaLoader.java:loadSchema(171)) - Loading network topology layer schema file
2019-09-26 20:33:19,145 [main] INFO  node.SCMNodeManager (SCMNodeManager.java:<init>(114)) - Entering startup safe mode.
2019-09-26 20:33:19,220 [main] INFO  algorithms.ContainerPlacementPolicyFactory (ContainerPlacementPolicyFactory.java:getPolicy(57)) - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
2019-09-26 20:33:19,235 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-26 20:33:19,297 [main] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:initializePipelineState(132)) - No pipeline exists in current db
2019-09-26 20:33:19,301 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-26 20:33:19,383 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for hdds.scm.replication.thread.interval(2000) assuming MILLISECONDS
2019-09-26 20:33:19,413 [main] WARN  events.EventQueue (EventQueue.java:fireEvent(183)) - No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='SafeModeStatus'}
ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
2019-09-26 20:33:19,779 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-26 20:33:19,807 [Socket Reader #1 for port 33697] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 33697
2019-09-26 20:33:19,947 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-26 20:33:19,948 [Socket Reader #1 for port 39985] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 39985
2019-09-26 20:33:19,958 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-26 20:33:19,959 [Socket Reader #1 for port 46209] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 46209
2019-09-26 20:33:19,982 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for scm at: http://0.0.0.0:0
2019-09-26 20:33:20,126 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-26 20:33:20,133 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-26 20:33:20,141 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-26 20:33:20,144 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
2019-09-26 20:33:20,144 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-26 20:33:20,145 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-26 20:33:20,176 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(770)) - StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:46209
2019-09-26 20:33:20,263 [main] WARN  impl.MetricsConfig (MetricsConfig.java:loadFirst(134)) - Cannot locate configuration: tried hadoop-metrics2-storagecontainermanager.properties,hadoop-metrics2.properties
2019-09-26 20:33:20,281 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 10 second(s).
2019-09-26 20:33:20,281 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - StorageContainerManager metrics system started
2019-09-26 20:33:20,540 [main] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:start(151)) - RPC server for Client  is listening at /0.0.0.0:46209
2019-09-26 20:33:20,541 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-26 20:33:20,541 [IPC Server listener on 46209] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 46209: starting
2019-09-26 20:33:20,544 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(780)) - ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:39985
2019-09-26 20:33:20,545 [main] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:start(146)) - RPC server for Block Protocol is listening at /0.0.0.0:39985
2019-09-26 20:33:20,545 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-26 20:33:20,546 [IPC Server listener on 39985] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 39985: starting
2019-09-26 20:33:20,548 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(784)) - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:33697
2019-09-26 20:33:20,548 [main] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:start(191)) - RPC server for DataNodes is listening at /0.0.0.0:33697
2019-09-26 20:33:20,548 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-26 20:33:20,548 [IPC Server listener on 33697] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 33697: starting
2019-09-26 20:33:20,553 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 35031
2019-09-26 20:33:20,555 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-26 20:33:20,595 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6b00f608{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-26 20:33:20,596 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3e821657{/static,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/static,AVAILABLE}
2019-09-26 20:33:20,630 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@6928f576{/,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/scm/,AVAILABLE}{/scm}
2019-09-26 20:33:20,636 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@548e76f1{HTTP/1.1,[http/1.1]}{0.0.0.0:35031}
2019-09-26 20:33:20,636 [main] INFO  server.Server (Server.java:doStart(419)) - Started @2889ms
2019-09-26 20:33:20,638 [main] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:start(204)) - Sink prometheus started
2019-09-26 20:33:20,638 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:registerSink(301)) - Registered sink prometheus
2019-09-26 20:33:20,640 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of SCM is listening at http://0.0.0.0:35031
2019-09-26 20:33:20,646 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@73511076] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-26 20:33:20,650 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-26 20:33:20,786 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:loadOMHAConfig(194)) - Configuration either no ozone.om.address set. Falling back to the default OM address /127.0.0.1:0
2019-09-26 20:33:20,786 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:getOMNodeDetails(224)) - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
2019-09-26 20:33:20,788 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-26 20:33:20,789 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-26 20:33:21,524 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-26 20:33:21,533 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: userTable
2019-09-26 20:33:21,533 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:userTable
2019-09-26 20:33:21,533 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: volumeTable
2019-09-26 20:33:21,534 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:volumeTable
2019-09-26 20:33:21,534 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: bucketTable
2019-09-26 20:33:21,534 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:bucketTable
2019-09-26 20:33:21,535 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: keyTable
2019-09-26 20:33:21,535 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:keyTable
2019-09-26 20:33:21,536 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: deletedTable
2019-09-26 20:33:21,536 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:deletedTable
2019-09-26 20:33:21,536 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: openKeyTable
2019-09-26 20:33:21,537 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:openKeyTable
2019-09-26 20:33:21,537 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: s3Table
2019-09-26 20:33:21,537 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:s3Table
2019-09-26 20:33:21,538 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: multipartInfoTable
2019-09-26 20:33:21,538 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:multipartInfoTable
2019-09-26 20:33:21,539 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: dTokenTable
2019-09-26 20:33:21,539 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:dTokenTable
2019-09-26 20:33:21,539 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: s3SecretTable
2019-09-26 20:33:21,540 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:s3SecretTable
2019-09-26 20:33:21,540 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: prefixTable
2019-09-26 20:33:21,540 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:prefixTable
2019-09-26 20:33:21,541 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: default
2019-09-26 20:33:21,541 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(167)) - Using default column profile:DBProfile.DISK for Table:default
2019-09-26 20:33:21,541 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:getDbProfile(198)) - Using default options. DBProfile.DISK
2019-09-26 20:33:22,103 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-26 20:33:22,104 [Socket Reader #1 for port 43049] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 43049
2019-09-26 20:33:22,126 [main] INFO  om.OzoneManager (OzoneManager.java:start(1069)) - OzoneManager RPC server is listening at localhost/127.0.0.1:43049
2019-09-26 20:33:22,127 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - OzoneManager metrics system started (again)
2019-09-26 20:33:22,128 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-26 20:33:22,128 [IPC Server listener on 43049] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 43049: starting
2019-09-26 20:33:22,135 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for ozoneManager at: http://0.0.0.0:0
2019-09-26 20:33:22,137 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-26 20:33:22,137 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-26 20:33:22,140 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-26 20:33:22,141 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
2019-09-26 20:33:22,141 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-26 20:33:22,141 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-26 20:33:22,144 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 38469
2019-09-26 20:33:22,144 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-26 20:33:22,146 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3a627c80{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-26 20:33:22,147 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@963176{/static,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/static,AVAILABLE}
2019-09-26 20:33:22,155 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@4af46df3{/,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/ozoneManager/,AVAILABLE}{/ozoneManager}
2019-09-26 20:33:22,159 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4158debd{HTTP/1.1,[http/1.1]}{0.0.0.0:38469}
2019-09-26 20:33:22,161 [main] INFO  server.Server (Server.java:doStart(419)) - Started @4415ms
2019-09-26 20:33:22,162 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-26 20:33:22,163 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of OZONEMANAGER is listening at http://0.0.0.0:38469
2019-09-26 20:33:22,447 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-26 20:33:22,497 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-kpcbn-4093309294 ip:192.168.151.109
2019-09-26 20:33:22,528 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-26 20:33:22,530 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/containers/hdds to VolumeSet
2019-09-26 20:33:22,533 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@3fdecce
2019-09-26 20:33:22,551 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@3fdecce
2019-09-26 20:33:22,663 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-26 20:33:22,728 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-26 20:33:22,733 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-26 20:33:22,734 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-26 20:33:22,735 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-26 20:33:22,736 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-26 20:33:22,737 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-26 20:33:22,892 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/ratis] (custom)
2019-09-26 20:33:22,943 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-26 20:33:22,945 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-26 20:33:22,946 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-26 20:33:22,948 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-26 20:33:22,949 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-26 20:33:22,950 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-26 20:33:22,950 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-26 20:33:22,951 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 46496
2019-09-26 20:33:22,951 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-26 20:33:22,953 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6d1dcdff{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-26 20:33:22,954 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7ff35a3f{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-26 20:33:22,998 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@a66e580{/,file:///tmp/jetty-0.0.0.0-46496-hddsDatanode-_-any-5915977255003974097.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-26 20:33:22,999 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@5b852b49{HTTP/1.1,[http/1.1]}{0.0.0.0:46496}
2019-09-26 20:33:23,000 [main] INFO  server.Server (Server.java:doStart(419)) - Started @5253ms
2019-09-26 20:33:23,000 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-26 20:33:23,001 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:46496
2019-09-26 20:33:23,003 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-26 20:33:23,006 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-kpcbn-4093309294 ip:192.168.151.109
2019-09-26 20:33:23,009 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@39e24e8e] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-26 20:33:23,014 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-1/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-26 20:33:23,015 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-1/data/containers/hdds to VolumeSet
2019-09-26 20:33:23,016 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@5bc28f40
2019-09-26 20:33:23,016 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@5bc28f40
2019-09-26 20:33:23,037 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-26 20:33:23,038 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-26 20:33:23,038 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-26 20:33:23,038 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-26 20:33:23,039 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-26 20:33:23,039 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-26 20:33:23,039 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-26 20:33:23,040 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-1/data/ratis] (custom)
2019-09-26 20:33:23,042 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-26 20:33:23,043 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-26 20:33:23,044 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-26 20:33:23,046 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-26 20:33:23,047 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-26 20:33:23,047 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-26 20:33:23,048 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-26 20:33:23,049 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 38463
2019-09-26 20:33:23,049 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-26 20:33:23,052 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6cd64ee8{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-26 20:33:23,053 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2f1d0bbc{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-26 20:33:23,090 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@59072e9d{/,file:///tmp/jetty-0.0.0.0-38463-hddsDatanode-_-any-7838931230963524665.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-26 20:33:23,091 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@58472096{HTTP/1.1,[http/1.1]}{0.0.0.0:38463}
2019-09-26 20:33:23,092 [main] INFO  server.Server (Server.java:doStart(419)) - Started @5345ms
2019-09-26 20:33:23,092 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-26 20:33:23,093 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:38463
2019-09-26 20:33:23,093 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-26 20:33:23,096 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@23b9ffab] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-26 20:33:23,096 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-kpcbn-4093309294 ip:192.168.151.109
2019-09-26 20:33:23,104 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-26 20:33:23,105 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/containers/hdds to VolumeSet
2019-09-26 20:33:23,105 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@6cc86152
2019-09-26 20:33:23,106 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@6cc86152
2019-09-26 20:33:23,110 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/meta/datanode.id
2019-09-26 20:33:23,115 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-1/meta/datanode.id
2019-09-26 20:33:23,123 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-26 20:33:23,123 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-26 20:33:23,124 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-26 20:33:23,124 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-26 20:33:23,124 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-26 20:33:23,124 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-26 20:33:23,125 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-26 20:33:23,125 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/ratis] (custom)
2019-09-26 20:33:23,127 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-26 20:33:23,128 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-26 20:33:23,129 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-26 20:33:23,130 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-26 20:33:23,131 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-26 20:33:23,131 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-26 20:33:23,131 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-26 20:33:23,132 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 46494
2019-09-26 20:33:23,132 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-26 20:33:23,134 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@77b919a3{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-26 20:33:23,134 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@36681447{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-26 20:33:23,169 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@12e0f1cb{/,file:///tmp/jetty-0.0.0.0-46494-hddsDatanode-_-any-5833751225197359135.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-26 20:33:23,171 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4a163575{HTTP/1.1,[http/1.1]}{0.0.0.0:46494}
2019-09-26 20:33:23,172 [main] INFO  server.Server (Server.java:doStart(419)) - Started @5426ms
2019-09-26 20:33:23,173 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-26 20:33:23,174 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:46494
2019-09-26 20:33:23,174 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-26 20:33:23,177 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@65b946b] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-26 20:33:23,178 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-kpcbn-4093309294 ip:192.168.151.109
2019-09-26 20:33:23,183 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/meta/datanode.id
2019-09-26 20:33:23,187 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-26 20:33:23,187 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/containers/hdds to VolumeSet
2019-09-26 20:33:23,187 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@6540cf1d
2019-09-26 20:33:23,188 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@6540cf1d
2019-09-26 20:33:23,204 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-26 20:33:23,204 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-26 20:33:23,204 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-26 20:33:23,205 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-26 20:33:23,205 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-26 20:33:23,205 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-26 20:33:23,205 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-26 20:33:23,206 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/ratis] (custom)
2019-09-26 20:33:23,208 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-26 20:33:23,209 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-26 20:33:23,209 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-26 20:33:23,211 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-26 20:33:23,212 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-26 20:33:23,212 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-26 20:33:23,212 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-26 20:33:23,213 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 33532
2019-09-26 20:33:23,213 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-26 20:33:23,217 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@378cfecf{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,AVAILABLE}
2019-09-26 20:33:23,218 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5e7c141d{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-26 20:33:23,247 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@437486cd{/,file:///tmp/jetty-0.0.0.0-33532-hddsDatanode-_-any-8764202420779431051.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-26 20:33:23,249 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@15b642b9{HTTP/1.1,[http/1.1]}{0.0.0.0:33532}
2019-09-26 20:33:23,250 [main] INFO  server.Server (Server.java:doStart(419)) - Started @5503ms
2019-09-26 20:33:23,250 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-26 20:33:23,251 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:33532
2019-09-26 20:33:23,253 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 4 DN Heartbeats.
2019-09-26 20:33:23,253 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4b55ebf6] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-26 20:33:23,255 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/meta/datanode.id
2019-09-26 20:33:24,253 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 4 DN Heartbeats.
2019-09-26 20:33:25,062 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-26 20:33:25,065 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-26 20:33:25,065 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis d4e52575-d0bc-4eb0-97d4-5097dba1d6da at port 0
2019-09-26 20:33:25,085 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: start RPC server
2019-09-26 20:33:25,114 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-26 20:33:25,118 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-26 20:33:25,118 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e at port 0
2019-09-26 20:33:25,124 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: start RPC server
2019-09-26 20:33:25,193 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-26 20:33:25,195 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-26 20:33:25,195 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis df1e4984-ea57-49fe-b366-a19f316192bd at port 0
2019-09-26 20:33:25,204 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - df1e4984-ea57-49fe-b366-a19f316192bd: start RPC server
2019-09-26 20:33:25,215 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: GrpcService started, listening on 0.0.0.0/0.0.0.0:43035
2019-09-26 20:33:25,215 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: GrpcService started, listening on 0.0.0.0/0.0.0.0:43952
2019-09-26 20:33:25,215 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - df1e4984-ea57-49fe-b366-a19f316192bd: GrpcService started, listening on 0.0.0.0/0.0.0.0:45194
2019-09-26 20:33:25,216 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis d4e52575-d0bc-4eb0-97d4-5097dba1d6da is started using port 43952
2019-09-26 20:33:25,216 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e is started using port 43035
2019-09-26 20:33:25,216 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis df1e4984-ea57-49fe-b366-a19f316192bd is started using port 45194
2019-09-26 20:33:25,221 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e is started using port 39052
2019-09-26 20:33:25,221 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc d4e52575-d0bc-4eb0-97d4-5097dba1d6da is started using port 44492
2019-09-26 20:33:25,221 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc df1e4984-ea57-49fe-b366-a19f316192bd is started using port 43838
2019-09-26 20:33:25,255 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 4 DN Heartbeats.
2019-09-26 20:33:25,269 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-26 20:33:25,271 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-26 20:33:25,271 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis 5caead7e-358b-4710-835a-2d2cbee8bfb7 at port 0
2019-09-26 20:33:25,276 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: start RPC server
2019-09-26 20:33:25,278 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: GrpcService started, listening on 0.0.0.0/0.0.0.0:46621
2019-09-26 20:33:25,278 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis 5caead7e-358b-4710-835a-2d2cbee8bfb7 is started using port 46621
2019-09-26 20:33:25,280 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc 5caead7e-358b-4710-835a-2d2cbee8bfb7 is started using port 39136
2019-09-26 20:33:26,255 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 4 DN Heartbeats.
2019-09-26 20:33:27,050 [IPC Server handler 0 on 33697] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/d4e52575-d0bc-4eb0-97d4-5097dba1d6da
2019-09-26 20:33:27,050 [IPC Server handler 0 on 33697] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : d4e52575-d0bc-4eb0-97d4-5097dba1d6da{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}
2019-09-26 20:33:27,056 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (DataNodeSafeModeRule.java:process(71)) - SCM in safe mode. 1 DataNodes registered, 1 required.
2019-09-26 20:33:27,056 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(177)) - ScmSafeModeManager, all rules are successfully validated
2019-09-26 20:33:27,056 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:exitSafeMode(193)) - SCM exiting safe mode.
2019-09-26 20:33:27,101 [IPC Server handler 2 on 33697] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/8f7164d2-134a-4bb2-9a18-5a7fc6abc41e
2019-09-26 20:33:27,101 [IPC Server handler 2 on 33697] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}
2019-09-26 20:33:27,180 [IPC Server handler 3 on 33697] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/df1e4984-ea57-49fe-b366-a19f316192bd
2019-09-26 20:33:27,180 [IPC Server handler 3 on 33697] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : df1e4984-ea57-49fe-b366-a19f316192bd{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}
2019-09-26 20:33:27,256 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 3 of 4 DN Heartbeats.
2019-09-26 20:33:27,256 [IPC Server handler 4 on 33697] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/5caead7e-358b-4710-835a-2d2cbee8bfb7
2019-09-26 20:33:27,256 [IPC Server handler 4 on 33697] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : 5caead7e-358b-4710-835a-2d2cbee8bfb7{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}
2019-09-26 20:33:27,575 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: addNew group-CAD149CF701A:[d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952] returns group-CAD149CF701A:java.util.concurrent.CompletableFuture@a4272aa[Not completed]
2019-09-26 20:33:27,591 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: new RaftServerImpl for group-CAD149CF701A:[d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952] with ContainerStateMachine:uninitialized
2019-09-26 20:33:27,594 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-26 20:33:27,596 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-26 20:33:27,596 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-26 20:33:27,597 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-26 20:33:27,598 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-26 20:33:27,606 [pool-27-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A: ConfigurationManager, init=-1: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952], old=null, confs=<EMPTY_MAP>
2019-09-26 20:33:27,607 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/ratis] (custom)
2019-09-26 20:33:27,616 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/ratis/df4205ee-2f8c-4168-bf02-cad149cf701a does not exist. Creating ...
2019-09-26 20:33:27,646 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/ratis/df4205ee-2f8c-4168-bf02-cad149cf701a/in_use.lock acquired by nodename 3079@pr-hdds-2162-kpcbn-4093309294
2019-09-26 20:33:27,657 [pool-27-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/ratis/df4205ee-2f8c-4168-bf02-cad149cf701a has been successfully formatted.
2019-09-26 20:33:27,660 [pool-27-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-CAD149CF701A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-26 20:33:27,660 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-26 20:33:27,662 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-26 20:33:27,668 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-26 20:33:27,668 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-26 20:33:27,670 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:27,674 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-26 20:33:27,680 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/ratis/df4205ee-2f8c-4168-bf02-cad149cf701a
2019-09-26 20:33:27,681 [pool-27-thread-1] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - ratis metrics system started (again)
2019-09-26 20:33:27,687 [pool-27-thread-1] INFO  metrics.MetricRegistries (MetricRegistriesLoader.java:load(64)) - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
2019-09-26 20:33:27,714 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-26 20:33:27,714 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-26 20:33:27,717 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:27,718 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-26 20:33:27,718 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-26 20:33:27,719 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-26 20:33:27,719 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-26 20:33:27,720 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-26 20:33:27,720 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-26 20:33:27,728 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-26 20:33:27,732 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-26 20:33:27,736 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-26 20:33:27,737 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-26 20:33:27,738 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-26 20:33:27,738 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-26 20:33:27,764 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A: start as a follower, conf=-1: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952], old=null
2019-09-26 20:33:27,765 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-26 20:33:27,766 [pool-27-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: start FollowerState
2019-09-26 20:33:27,768 [pool-27-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CAD149CF701A,id=d4e52575-d0bc-4eb0-97d4-5097dba1d6da
2019-09-26 20:33:27,840 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: df4205ee-2f8c-4168-bf02-cad149cf701a, Nodes: d4e52575-d0bc-4eb0-97d4-5097dba1d6da{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-26 20:33:27,866 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: addNew group-2855985434A4:[5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621] returns group-2855985434A4:java.util.concurrent.CompletableFuture@5d1e91c[Not completed]
2019-09-26 20:33:27,894 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: new RaftServerImpl for group-2855985434A4:[5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621] with ContainerStateMachine:uninitialized
2019-09-26 20:33:27,896 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-26 20:33:27,896 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-26 20:33:27,896 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-26 20:33:27,896 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-26 20:33:27,896 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-26 20:33:27,897 [pool-57-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4: ConfigurationManager, init=-1: [5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621], old=null, confs=<EMPTY_MAP>
2019-09-26 20:33:27,897 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/ratis] (custom)
2019-09-26 20:33:27,897 [pool-57-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/ratis/3e72e795-be68-4611-b8f9-2855985434a4 does not exist. Creating ...
2019-09-26 20:33:27,911 [pool-57-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/ratis/3e72e795-be68-4611-b8f9-2855985434a4/in_use.lock acquired by nodename 3079@pr-hdds-2162-kpcbn-4093309294
2019-09-26 20:33:27,924 [pool-57-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/ratis/3e72e795-be68-4611-b8f9-2855985434a4 has been successfully formatted.
2019-09-26 20:33:27,925 [pool-57-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-2855985434A4: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-26 20:33:27,926 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-26 20:33:27,926 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-26 20:33:27,926 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-26 20:33:27,926 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-26 20:33:27,927 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:27,927 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-26 20:33:27,927 [pool-57-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/ratis/3e72e795-be68-4611-b8f9-2855985434a4
2019-09-26 20:33:27,932 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-26 20:33:27,932 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-26 20:33:27,933 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:27,933 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-26 20:33:27,933 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-26 20:33:27,933 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-26 20:33:27,933 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-26 20:33:27,933 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-26 20:33:27,934 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-26 20:33:27,934 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-26 20:33:27,934 [pool-57-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-26 20:33:27,935 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-26 20:33:27,935 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-26 20:33:27,935 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-26 20:33:27,935 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-26 20:33:27,939 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4: start as a follower, conf=-1: [5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621], old=null
2019-09-26 20:33:27,939 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-26 20:33:27,940 [pool-57-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: start FollowerState
2019-09-26 20:33:27,941 [pool-57-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2855985434A4,id=5caead7e-358b-4710-835a-2d2cbee8bfb7
2019-09-26 20:33:27,954 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 3e72e795-be68-4611-b8f9-2855985434a4, Nodes: 5caead7e-358b-4710-835a-2d2cbee8bfb7{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-26 20:33:27,974 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - df1e4984-ea57-49fe-b366-a19f316192bd: addNew group-04096F287BF8:[df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194] returns group-04096F287BF8:java.util.concurrent.CompletableFuture@6de5405c[Not completed]
2019-09-26 20:33:27,988 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - df1e4984-ea57-49fe-b366-a19f316192bd: new RaftServerImpl for group-04096F287BF8:[df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194] with ContainerStateMachine:uninitialized
2019-09-26 20:33:27,991 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-26 20:33:27,991 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-26 20:33:27,991 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-26 20:33:27,991 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-26 20:33:27,992 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-26 20:33:27,992 [pool-47-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8: ConfigurationManager, init=-1: [df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null, confs=<EMPTY_MAP>
2019-09-26 20:33:27,992 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/ratis] (custom)
2019-09-26 20:33:27,993 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/ratis/e103b091-44ec-4153-a353-04096f287bf8 does not exist. Creating ...
2019-09-26 20:33:28,005 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/ratis/e103b091-44ec-4153-a353-04096f287bf8/in_use.lock acquired by nodename 3079@pr-hdds-2162-kpcbn-4093309294
2019-09-26 20:33:28,019 [pool-47-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/ratis/e103b091-44ec-4153-a353-04096f287bf8 has been successfully formatted.
2019-09-26 20:33:28,019 [pool-47-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-04096F287BF8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-26 20:33:28,019 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-26 20:33:28,019 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-26 20:33:28,020 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-26 20:33:28,020 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-26 20:33:28,020 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:28,020 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-26 20:33:28,020 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/ratis/e103b091-44ec-4153-a353-04096f287bf8
2019-09-26 20:33:28,040 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-26 20:33:28,041 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-26 20:33:28,041 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:28,041 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-26 20:33:28,041 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-26 20:33:28,041 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-26 20:33:28,042 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-26 20:33:28,042 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-26 20:33:28,042 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-26 20:33:28,042 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-26 20:33:28,043 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-26 20:33:28,043 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-26 20:33:28,043 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-26 20:33:28,043 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-26 20:33:28,044 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-26 20:33:28,048 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8: start as a follower, conf=-1: [df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null
2019-09-26 20:33:28,048 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-26 20:33:28,048 [pool-47-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - df1e4984-ea57-49fe-b366-a19f316192bd: start FollowerState
2019-09-26 20:33:28,048 [pool-47-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-04096F287BF8,id=df1e4984-ea57-49fe-b366-a19f316192bd
2019-09-26 20:33:28,056 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: e103b091-44ec-4153-a353-04096f287bf8, Nodes: df1e4984-ea57-49fe-b366-a19f316192bd{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-26 20:33:28,074 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: addNew group-12C2BDCF6179:[8f7164d2-134a-4bb2-9a18-5a7fc6abc41e:192.168.151.109:43035] returns group-12C2BDCF6179:java.util.concurrent.CompletableFuture@51372af4[Not completed]
2019-09-26 20:33:28,075 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: new RaftServerImpl for group-12C2BDCF6179:[8f7164d2-134a-4bb2-9a18-5a7fc6abc41e:192.168.151.109:43035] with ContainerStateMachine:uninitialized
2019-09-26 20:33:28,075 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-26 20:33:28,075 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-26 20:33:28,075 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-26 20:33:28,076 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-26 20:33:28,076 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-26 20:33:28,076 [pool-37-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179: ConfigurationManager, init=-1: [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e:192.168.151.109:43035], old=null, confs=<EMPTY_MAP>
2019-09-26 20:33:28,076 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-1/data/ratis] (custom)
2019-09-26 20:33:28,077 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-1/data/ratis/0bf88db5-b565-4fd4-b518-12c2bdcf6179 does not exist. Creating ...
2019-09-26 20:33:28,090 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-1/data/ratis/0bf88db5-b565-4fd4-b518-12c2bdcf6179/in_use.lock acquired by nodename 3079@pr-hdds-2162-kpcbn-4093309294
2019-09-26 20:33:28,108 [pool-37-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-1/data/ratis/0bf88db5-b565-4fd4-b518-12c2bdcf6179 has been successfully formatted.
2019-09-26 20:33:28,108 [pool-37-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-12C2BDCF6179: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-26 20:33:28,108 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-26 20:33:28,109 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-26 20:33:28,109 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-26 20:33:28,109 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-26 20:33:28,109 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:28,109 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-26 20:33:28,109 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-1/data/ratis/0bf88db5-b565-4fd4-b518-12c2bdcf6179
2019-09-26 20:33:28,113 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-26 20:33:28,113 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-26 20:33:28,113 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:28,114 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-26 20:33:28,114 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-26 20:33:28,114 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-26 20:33:28,114 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-26 20:33:28,114 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-26 20:33:28,114 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-26 20:33:28,114 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-26 20:33:28,115 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-26 20:33:28,115 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-26 20:33:28,115 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-26 20:33:28,115 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-26 20:33:28,116 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-26 20:33:28,119 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179: start as a follower, conf=-1: [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e:192.168.151.109:43035], old=null
2019-09-26 20:33:28,119 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-26 20:33:28,119 [pool-37-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: start FollowerState
2019-09-26 20:33:28,120 [pool-37-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-12C2BDCF6179,id=8f7164d2-134a-4bb2-9a18-5a7fc6abc41e
2019-09-26 20:33:28,128 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 0bf88db5-b565-4fd4-b518-12c2bdcf6179, Nodes: 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-26 20:33:28,172 [grpc-default-executor-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: addNew group-52FE9A5D9B07:[d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194] returns group-52FE9A5D9B07:java.util.concurrent.CompletableFuture@4b1949ae[Not completed]
2019-09-26 20:33:28,175 [grpc-default-executor-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - df1e4984-ea57-49fe-b366-a19f316192bd: addNew group-52FE9A5D9B07:[d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194] returns group-52FE9A5D9B07:java.util.concurrent.CompletableFuture@46a4ea66[Not completed]
2019-09-26 20:33:28,175 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: addNew group-52FE9A5D9B07:[d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194] returns group-52FE9A5D9B07:java.util.concurrent.CompletableFuture@47df897c[Not completed]
2019-09-26 20:33:28,175 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: new RaftServerImpl for group-52FE9A5D9B07:[d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194] with ContainerStateMachine:uninitialized
2019-09-26 20:33:28,176 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-26 20:33:28,176 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-26 20:33:28,176 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-26 20:33:28,176 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-26 20:33:28,176 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-26 20:33:28,177 [pool-27-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07: ConfigurationManager, init=-1: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null, confs=<EMPTY_MAP>
2019-09-26 20:33:28,177 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/ratis] (custom)
2019-09-26 20:33:28,177 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: new RaftServerImpl for group-52FE9A5D9B07:[d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194] with ContainerStateMachine:uninitialized
2019-09-26 20:33:28,177 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-26 20:33:28,177 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07 does not exist. Creating ...
2019-09-26 20:33:28,177 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-26 20:33:28,178 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-26 20:33:28,178 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-26 20:33:28,178 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-26 20:33:28,178 [pool-57-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07: ConfigurationManager, init=-1: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null, confs=<EMPTY_MAP>
2019-09-26 20:33:28,178 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/ratis] (custom)
2019-09-26 20:33:28,179 [pool-57-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07 does not exist. Creating ...
2019-09-26 20:33:28,179 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - df1e4984-ea57-49fe-b366-a19f316192bd: new RaftServerImpl for group-52FE9A5D9B07:[d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194] with ContainerStateMachine:uninitialized
2019-09-26 20:33:28,179 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-26 20:33:28,179 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-26 20:33:28,180 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 1000s (custom)
2019-09-26 20:33:28,180 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-26 20:33:28,180 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-26 20:33:28,180 [pool-47-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07: ConfigurationManager, init=-1: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null, confs=<EMPTY_MAP>
2019-09-26 20:33:28,180 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/ratis] (custom)
2019-09-26 20:33:28,181 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07 does not exist. Creating ...
2019-09-26 20:33:28,192 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07/in_use.lock acquired by nodename 3079@pr-hdds-2162-kpcbn-4093309294
2019-09-26 20:33:28,192 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07/in_use.lock acquired by nodename 3079@pr-hdds-2162-kpcbn-4093309294
2019-09-26 20:33:28,192 [pool-57-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07/in_use.lock acquired by nodename 3079@pr-hdds-2162-kpcbn-4093309294
2019-09-26 20:33:28,204 [pool-27-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07 has been successfully formatted.
2019-09-26 20:33:28,204 [pool-47-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07 has been successfully formatted.
2019-09-26 20:33:28,204 [pool-57-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07 has been successfully formatted.
2019-09-26 20:33:28,204 [pool-27-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-52FE9A5D9B07: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-26 20:33:28,204 [pool-47-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-52FE9A5D9B07: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-26 20:33:28,204 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-26 20:33:28,204 [pool-57-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-52FE9A5D9B07: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-26 20:33:28,205 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-26 20:33:28,205 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-26 20:33:28,205 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-26 20:33:28,205 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 1000s (custom)
2019-09-26 20:33:28,205 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-26 20:33:28,205 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-26 20:33:28,205 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:28,205 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-26 20:33:28,206 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-26 20:33:28,206 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-26 20:33:28,206 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07
2019-09-26 20:33:28,206 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-26 20:33:28,206 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-26 20:33:28,206 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-26 20:33:28,206 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-26 20:33:28,206 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-26 20:33:28,207 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:28,206 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:28,207 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-26 20:33:28,207 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:28,207 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-26 20:33:28,207 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-26 20:33:28,208 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-26 20:33:28,208 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-26 20:33:28,208 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-26 20:33:28,208 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07
2019-09-26 20:33:28,208 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-26 20:33:28,208 [pool-57-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07
2019-09-26 20:33:28,209 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-26 20:33:28,208 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-26 20:33:28,209 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-26 20:33:28,209 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-26 20:33:28,209 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-26 20:33:28,210 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-26 20:33:28,210 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-26 20:33:28,210 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:28,210 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-26 20:33:28,210 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-26 20:33:28,210 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-26 20:33:28,211 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-26 20:33:28,210 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-26 20:33:28,211 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-26 20:33:28,211 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-26 20:33:28,211 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-26 20:33:28,211 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-26 20:33:28,211 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-26 20:33:28,211 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-26 20:33:28,212 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-26 20:33:28,212 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-26 20:33:28,212 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-26 20:33:28,212 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-26 20:33:28,212 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-26 20:33:28,213 [pool-57-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-26 20:33:28,213 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-26 20:33:28,213 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-26 20:33:28,213 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-26 20:33:28,213 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-26 20:33:28,214 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-26 20:33:28,214 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-26 20:33:28,214 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-26 20:33:28,214 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-26 20:33:28,214 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-26 20:33:28,214 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-26 20:33:28,215 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-26 20:33:28,215 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-26 20:33:28,219 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07: start as a follower, conf=-1: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null
2019-09-26 20:33:28,219 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-26 20:33:28,219 [pool-27-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: start FollowerState
2019-09-26 20:33:28,220 [pool-27-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-52FE9A5D9B07,id=d4e52575-d0bc-4eb0-97d4-5097dba1d6da
2019-09-26 20:33:28,220 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07: start as a follower, conf=-1: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null
2019-09-26 20:33:28,220 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07: start as a follower, conf=-1: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null
2019-09-26 20:33:28,220 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-26 20:33:28,221 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-26 20:33:28,221 [pool-57-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: start FollowerState
2019-09-26 20:33:28,221 [pool-47-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - df1e4984-ea57-49fe-b366-a19f316192bd: start FollowerState
2019-09-26 20:33:28,222 [pool-57-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-52FE9A5D9B07,id=5caead7e-358b-4710-835a-2d2cbee8bfb7
2019-09-26 20:33:28,223 [pool-47-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-52FE9A5D9B07,id=df1e4984-ea57-49fe-b366-a19f316192bd
2019-09-26 20:33:28,243 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 7ae74b1f-d41a-4575-b405-52fe9a5d9b07, Nodes: 5caead7e-358b-4710-835a-2d2cbee8bfb7{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}d4e52575-d0bc-4eb0-97d4-5097dba1d6da{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}df1e4984-ea57-49fe-b366-a19f316192bd{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-09-26 20:33:28,256 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Cluster is ready. Got 4 of 4 DN Heartbeats.
2019-09-26 20:33:28,260 [main] INFO  container.ReplicationManager (ReplicationManager.java:start(162)) - Starting Replication Monitor Thread.
2019-09-26 20:33:28,262 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 1 milliseconds for processing 0 containers.
2019-09-26 20:33:29,440 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:30,058 [Thread-193] INFO  container.ReplicationManager (ReplicationManager.java:start(169)) - Replication Monitor Thread is already running.
2019-09-26 20:33:30,263 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:30,441 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:31,443 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:32,264 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:32,444 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:32,839 [Thread-195] INFO  impl.FollowerState (FollowerState.java:run(106)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da:group-CAD149CF701A changes to CANDIDATE, lastRpcTime:5072, electionTimeout:5072ms
2019-09-26 20:33:32,841 [Thread-195] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: shutdown FollowerState
2019-09-26 20:33:32,841 [Thread-195] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-26 20:33:32,848 [Thread-195] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: start LeaderElection
2019-09-26 20:33:32,969 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1: begin an election at term 1 for -1: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952], old=null
2019-09-26 20:33:32,971 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: shutdown LeaderElection
2019-09-26 20:33:32,972 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-26 20:33:32,972 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A: change Leader from null to d4e52575-d0bc-4eb0-97d4-5097dba1d6da at term 1 for becomeLeader, leader elected after 5312ms
2019-09-26 20:33:32,980 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-26 20:33:32,981 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-26 20:33:32,984 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-26 20:33:32,988 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-26 20:33:32,988 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-26 20:33:32,989 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-26 20:33:33,005 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: start LeaderState
2019-09-26 20:33:33,028 [Thread-198] INFO  impl.FollowerState (FollowerState.java:run(106)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7:group-2855985434A4 changes to CANDIDATE, lastRpcTime:5088, electionTimeout:5085ms
2019-09-26 20:33:33,029 [Thread-198] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: shutdown FollowerState
2019-09-26 20:33:33,029 [Thread-198] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-26 20:33:33,030 [Thread-198] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: start LeaderElection
2019-09-26 20:33:33,036 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-26 20:33:33,048 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2: begin an election at term 1 for -1: [5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621], old=null
2019-09-26 20:33:33,050 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:LeaderElection1] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A: set configuration 0: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952], old=null at 0
2019-09-26 20:33:33,050 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: shutdown LeaderElection
2019-09-26 20:33:33,051 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-26 20:33:33,052 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4: change Leader from null to 5caead7e-358b-4710-835a-2d2cbee8bfb7 at term 1 for becomeLeader, leader elected after 5125ms
2019-09-26 20:33:33,052 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-26 20:33:33,052 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-26 20:33:33,052 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-26 20:33:33,052 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-26 20:33:33,053 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-26 20:33:33,053 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-26 20:33:33,053 [Thread-201] INFO  impl.FollowerState (FollowerState.java:run(106)) - df1e4984-ea57-49fe-b366-a19f316192bd:group-04096F287BF8 changes to CANDIDATE, lastRpcTime:5005, electionTimeout:5005ms
2019-09-26 20:33:33,054 [Thread-201] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - df1e4984-ea57-49fe-b366-a19f316192bd: shutdown FollowerState
2019-09-26 20:33:33,054 [Thread-201] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-26 20:33:33,054 [Thread-201] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - df1e4984-ea57-49fe-b366-a19f316192bd: start LeaderElection
2019-09-26 20:33:33,059 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: start LeaderState
2019-09-26 20:33:33,059 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-26 20:33:33,060 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:LeaderElection2] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4: set configuration 0: [5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621], old=null at 0
2019-09-26 20:33:33,070 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3: begin an election at term 1 for -1: [df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null
2019-09-26 20:33:33,071 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - df1e4984-ea57-49fe-b366-a19f316192bd: shutdown LeaderElection
2019-09-26 20:33:33,071 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-26 20:33:33,071 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8: change Leader from null to df1e4984-ea57-49fe-b366-a19f316192bd at term 1 for becomeLeader, leader elected after 5051ms
2019-09-26 20:33:33,072 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-26 20:33:33,073 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-26 20:33:33,073 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-26 20:33:33,073 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-26 20:33:33,073 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-26 20:33:33,073 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-26 20:33:33,077 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - df1e4984-ea57-49fe-b366-a19f316192bd: start LeaderState
2019-09-26 20:33:33,078 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-26 20:33:33,078 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:LeaderElection3] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8: set configuration 0: [df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null at 0
2019-09-26 20:33:33,203 [Thread-204] INFO  impl.FollowerState (FollowerState.java:run(106)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e:group-12C2BDCF6179 changes to CANDIDATE, lastRpcTime:5083, electionTimeout:5083ms
2019-09-26 20:33:33,203 [Thread-204] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: shutdown FollowerState
2019-09-26 20:33:33,203 [Thread-204] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-26 20:33:33,204 [Thread-204] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: start LeaderElection
2019-09-26 20:33:33,244 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/ratis/e103b091-44ec-4153-a353-04096f287bf8/current/log_inprogress_0
2019-09-26 20:33:33,244 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4: begin an election at term 1 for -1: [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e:192.168.151.109:43035], old=null
2019-09-26 20:33:33,244 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/ratis/3e72e795-be68-4611-b8f9-2855985434a4/current/log_inprogress_0
2019-09-26 20:33:33,244 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/ratis/df4205ee-2f8c-4168-bf02-cad149cf701a/current/log_inprogress_0
2019-09-26 20:33:33,245 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: shutdown LeaderElection
2019-09-26 20:33:33,245 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-26 20:33:33,245 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179: change Leader from null to 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e at term 1 for becomeLeader, leader elected after 5136ms
2019-09-26 20:33:33,248 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-26 20:33:33,248 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-26 20:33:33,248 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-26 20:33:33,249 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-26 20:33:33,249 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-26 20:33:33,249 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-26 20:33:33,252 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: start LeaderState
2019-09-26 20:33:33,252 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-26 20:33:33,252 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:LeaderElection4] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179: set configuration 0: [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e:192.168.151.109:43035], old=null at 0
2019-09-26 20:33:33,288 [Thread-211] INFO  impl.FollowerState (FollowerState.java:run(106)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7:group-52FE9A5D9B07 changes to CANDIDATE, lastRpcTime:5067, electionTimeout:5035ms
2019-09-26 20:33:33,288 [Thread-211] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: shutdown FollowerState
2019-09-26 20:33:33,288 [Thread-211] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-26 20:33:33,289 [Thread-211] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: start LeaderElection
2019-09-26 20:33:33,301 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-1/data/ratis/0bf88db5-b565-4fd4-b518-12c2bdcf6179/current/log_inprogress_0
2019-09-26 20:33:33,301 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5: begin an election at term 1 for -1: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null
2019-09-26 20:33:33,334 [grpc-default-executor-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:5caead7e-358b-4710-835a-2d2cbee8bfb7
2019-09-26 20:33:33,334 [grpc-default-executor-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: shutdown FollowerState
2019-09-26 20:33:33,334 [grpc-default-executor-0] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: start FollowerState
2019-09-26 20:33:33,334 [grpc-default-executor-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:5caead7e-358b-4710-835a-2d2cbee8bfb7
2019-09-26 20:33:33,335 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - df1e4984-ea57-49fe-b366-a19f316192bd: shutdown FollowerState
2019-09-26 20:33:33,335 [Thread-209] INFO  impl.FollowerState (FollowerState.java:run(115)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-26 20:33:33,335 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - df1e4984-ea57-49fe-b366-a19f316192bd: start FollowerState
2019-09-26 20:33:33,335 [Thread-212] INFO  impl.FollowerState (FollowerState.java:run(115)) - df1e4984-ea57-49fe-b366-a19f316192bd: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-26 20:33:33,364 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(56)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5: Election PASSED; received 1 response(s) [5caead7e-358b-4710-835a-2d2cbee8bfb7<-df1e4984-ea57-49fe-b366-a19f316192bd#0:OK-t1] and 0 exception(s); 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:t1, leader=null, voted=5caead7e-358b-4710-835a-2d2cbee8bfb7, raftlog=5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null
2019-09-26 20:33:33,364 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: shutdown LeaderElection
2019-09-26 20:33:33,364 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-26 20:33:33,365 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07: change Leader from null to 5caead7e-358b-4710-835a-2d2cbee8bfb7 at term 1 for becomeLeader, leader elected after 5160ms
2019-09-26 20:33:33,366 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-26 20:33:33,366 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-26 20:33:33,366 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-26 20:33:33,366 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-26 20:33:33,367 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-26 20:33:33,367 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-26 20:33:33,373 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2019-09-26 20:33:33,374 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-26 20:33:33,374 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2019-09-26 20:33:33,379 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2019-09-26 20:33:33,384 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-26 20:33:33,384 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-26 20:33:33,385 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2019-09-26 20:33:33,385 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-26 20:33:33,386 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2019-09-26 20:33:33,386 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2019-09-26 20:33:33,386 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-26 20:33:33,386 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-26 20:33:33,389 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: start LeaderState
2019-09-26 20:33:33,389 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-26 20:33:33,390 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:LeaderElection5] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07: set configuration 0: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null at 0
2019-09-26 20:33:33,431 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07/current/log_inprogress_0
2019-09-26 20:33:33,446 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:33,451 [grpc-default-executor-0] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07: change Leader from null to 5caead7e-358b-4710-835a-2d2cbee8bfb7 at term 1 for appendEntries, leader elected after 5246ms
2019-09-26 20:33:33,451 [grpc-default-executor-1] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07: change Leader from null to 5caead7e-358b-4710-835a-2d2cbee8bfb7 at term 1 for appendEntries, leader elected after 5246ms
2019-09-26 20:33:33,480 [grpc-default-executor-0] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07: set configuration 0: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null at 0
2019-09-26 20:33:33,480 [grpc-default-executor-1] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07: set configuration 0: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null at 0
2019-09-26 20:33:33,480 [grpc-default-executor-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-26 20:33:33,480 [grpc-default-executor-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-26 20:33:33,532 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07/current/log_inprogress_0
2019-09-26 20:33:33,532 [df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/ratis/7ae74b1f-d41a-4575-b405-52fe9a5d9b07/current/log_inprogress_0
2019-09-26 20:33:34,264 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:34,451 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:35,452 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:36,265 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:36,453 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:37,455 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:38,265 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:38,456 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:39,461 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:40,266 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:40,462 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:41,464 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:42,266 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:42,465 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:43,466 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:44,267 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:44,467 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:45,469 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:46,267 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:46,470 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:47,472 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:48,267 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:48,473 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:48,474 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 1 failover attempts. Trying to failover immediately.
2019-09-26 20:33:49,475 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:50,268 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:50,477 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:51,478 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:52,268 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:52,480 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:53,481 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:54,269 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:54,483 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:55,484 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:56,269 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:56,485 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:57,486 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:58,270 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:33:58,488 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:33:58,489 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 2 failover attempts. Trying to failover immediately.
2019-09-26 20:33:59,490 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:00,270 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:00,491 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:01,493 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:02,270 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:02,494 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:03,495 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:04,271 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:04,497 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:05,498 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:06,272 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:06,499 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:07,501 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:08,272 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:08,502 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:08,503 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 3 failover attempts. Trying to failover immediately.
2019-09-26 20:34:09,505 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:10,273 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:10,506 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:11,508 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:12,273 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:12,509 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:13,510 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:14,273 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:14,512 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:15,513 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:16,274 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:16,514 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:17,515 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:18,275 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:18,516 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:18,518 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 4 failover attempts. Trying to failover immediately.
2019-09-26 20:34:19,520 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:20,275 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:20,521 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:21,522 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:22,276 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:22,523 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:23,524 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:24,276 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:24,525 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:25,527 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:26,276 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:26,528 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:27,529 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:28,277 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:28,530 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:28,532 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 5 failover attempts. Trying to failover immediately.
2019-09-26 20:34:29,533 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:30,277 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:30,535 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:31,536 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:32,278 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 1 milliseconds for processing 0 containers.
2019-09-26 20:34:32,537 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:33,538 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:34,278 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:34,539 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:35,541 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:36,279 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:36,542 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:37,543 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:38,279 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:38,544 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:38,546 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 6 failover attempts. Trying to failover immediately.
2019-09-26 20:34:39,547 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:40,280 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:40,548 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:41,549 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:42,280 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:42,551 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:43,552 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:44,281 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:44,553 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:45,554 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:46,281 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:46,555 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:47,556 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:48,281 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:48,557 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:48,559 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 7 failover attempts. Trying to failover immediately.
2019-09-26 20:34:49,560 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:50,282 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:50,561 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:51,562 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:52,283 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:52,563 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:53,564 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:54,283 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:54,566 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:55,567 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:56,284 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:56,568 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:57,569 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:58,284 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:34:58,570 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:34:58,572 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 8 failover attempts. Trying to failover immediately.
2019-09-26 20:34:59,573 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:00,285 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:00,578 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:01,582 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:02,285 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:02,583 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:03,584 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:04,285 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:04,585 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:05,586 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:06,286 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:06,587 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:07,588 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:08,286 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:08,590 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:08,591 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 9 failover attempts. Trying to failover immediately.
2019-09-26 20:35:09,592 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:10,287 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:10,593 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:11,594 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:12,287 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:12,596 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:13,597 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:14,287 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:14,598 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:15,599 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:16,288 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:16,601 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:17,602 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:18,288 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:18,603 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-26 20:35:18,605 [main] ERROR ha.OMFailoverProxyProvider (OzoneManagerProtocolClientSideTranslatorPB.java:getRetryAction(268)) - Failed to connect to OM. Attempted 10 retries and 10 failovers
2019-09-26 20:35:18,610 [main] ERROR client.OzoneClientFactory (OzoneClientFactory.java:getClientProtocol(259)) - Couldn't create RpcClient protocol exception: 
java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy36.submitRequest(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy36.submitRequest(Unknown Source)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:338)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1223)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
	at com.sun.proxy.$Proxy37.getServiceInfo(Unknown Source)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:154)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:256)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:239)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClient(OzoneClientFactory.java:75)
	at org.apache.hadoop.ozone.client.rpc.TestContainerReplicationEndToEnd.init(TestContainerReplicationEndToEnd.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 45 more
2019-09-26 20:35:18,614 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:shutdown(314)) - Shutting down the Mini Ozone Cluster
2019-09-26 20:35:18,615 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stop(330)) - Stopping the Mini Ozone Cluster
2019-09-26 20:35:18,615 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopOM(400)) - Stopping the OzoneManager
2019-09-26 20:35:18,615 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 43049
2019-09-26 20:35:18,620 [IPC Server listener on 43049] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 43049
2019-09-26 20:35:18,622 [main] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:stop(248)) - Stopping OMDoubleBuffer flush thread
2019-09-26 20:35:18,626 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-26 20:35:18,628 [OMDoubleBufferFlushThread] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:flushTransactions(191)) - OMDoubleBuffer flush thread OMDoubleBufferFlushThread is interrupted and will exit. OMDoubleBufferFlushThread
2019-09-26 20:35:18,639 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service KeyDeletingService
2019-09-26 20:35:18,650 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@4af46df3{/,null,UNAVAILABLE}{/ozoneManager}
2019-09-26 20:35:18,656 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4158debd{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-26 20:35:18,656 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@963176{/static,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/static,UNAVAILABLE}
2019-09-26 20:35:18,657 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3a627c80{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-26 20:35:18,662 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopDatanodes(377)) - Stopping the HddsDatanodes
2019-09-26 20:35:18,719 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-26 20:35:18,785 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-26 20:35:20,289 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:20,575 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode df1e4984-ea57-49fe-b366-a19f316192bd{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=7ae74b1f-d41a-4575-b405-52fe9a5d9b07, PipelineID=e103b091-44ec-4153-a353-04096f287bf8]
2019-09-26 20:35:20,575 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 7ae74b1f-d41a-4575-b405-52fe9a5d9b07, Nodes: 5caead7e-358b-4710-835a-2d2cbee8bfb7{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}d4e52575-d0bc-4eb0-97d4-5097dba1d6da{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}df1e4984-ea57-49fe-b366-a19f316192bd{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-09-26 20:35:20,576 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:finalizePipeline(123)) - Pipeline Pipeline[ Id: 7ae74b1f-d41a-4575-b405-52fe9a5d9b07, Nodes: 5caead7e-358b-4710-835a-2d2cbee8bfb7{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}d4e52575-d0bc-4eb0-97d4-5097dba1d6da{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}df1e4984-ea57-49fe-b366-a19f316192bd{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED] moved to CLOSED state
2019-09-26 20:35:20,578 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: e103b091-44ec-4153-a353-04096f287bf8, Nodes: df1e4984-ea57-49fe-b366-a19f316192bd{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-26 20:35:20,582 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:finalizePipeline(123)) - Pipeline Pipeline[ Id: e103b091-44ec-4153-a353-04096f287bf8, Nodes: df1e4984-ea57-49fe-b366-a19f316192bd{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] moved to CLOSED state
2019-09-26 20:35:20,675 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=0bf88db5-b565-4fd4-b518-12c2bdcf6179]
2019-09-26 20:35:20,675 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 0bf88db5-b565-4fd4-b518-12c2bdcf6179, Nodes: 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-26 20:35:20,675 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:finalizePipeline(123)) - Pipeline Pipeline[ Id: 0bf88db5-b565-4fd4-b518-12c2bdcf6179, Nodes: 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] moved to CLOSED state
2019-09-26 20:35:22,289 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:22,579 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 7ae74b1f-d41a-4575-b405-52fe9a5d9b07, Nodes: 5caead7e-358b-4710-835a-2d2cbee8bfb7{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}d4e52575-d0bc-4eb0-97d4-5097dba1d6da{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}df1e4984-ea57-49fe-b366-a19f316192bd{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED]
2019-09-26 20:35:22,602 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: remove    LEADER 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07:t1, leader=5caead7e-358b-4710-835a-2d2cbee8bfb7, voted=5caead7e-358b-4710-835a-2d2cbee8bfb7, raftlog=5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null RUNNING
2019-09-26 20:35:22,606 [grpc-default-executor-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07: shutdown
2019-09-26 20:35:22,606 [grpc-default-executor-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-52FE9A5D9B07,id=5caead7e-358b-4710-835a-2d2cbee8bfb7
2019-09-26 20:35:22,607 [grpc-default-executor-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: shutdown LeaderState
2019-09-26 20:35:22,609 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$414/797207686@44d65fd7] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(143)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07->d4e52575-d0bc-4eb0-97d4-5097dba1d6da-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2019-09-26 20:35:22,611 [grpc-default-executor-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7-PendingRequests: sendNotLeaderResponses
2019-09-26 20:35:22,609 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$414/797207686@1c2f0ef1] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(143)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07->df1e4984-ea57-49fe-b366-a19f316192bd-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2019-09-26 20:35:22,615 [grpc-default-executor-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07-StateMachineUpdater: set stopIndex = 0
2019-09-26 20:35:22,617 [grpc-default-executor-1] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(113)) - df1e4984-ea57-49fe-b366-a19f316192bd: Completed APPEND_ENTRIES, lastRequest: 5caead7e-358b-4710-835a-2d2cbee8bfb7->df1e4984-ea57-49fe-b366-a19f316192bd#44-t1, previous=(t:1, i:0), leaderCommit=0, initializing? false, entries: <empty>
2019-09-26 20:35:22,618 [grpc-default-executor-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07: closes. applyIndex: 0
2019-09-26 20:35:22,617 [grpc-default-executor-3] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(113)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: Completed APPEND_ENTRIES, lastRequest: 5caead7e-358b-4710-835a-2d2cbee8bfb7->d4e52575-d0bc-4eb0-97d4-5097dba1d6da#44-t1, previous=(t:1, i:0), leaderCommit=0, initializing? false, entries: <empty>
2019-09-26 20:35:22,620 [grpc-default-executor-3] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(293)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07->d4e52575-d0bc-4eb0-97d4-5097dba1d6da-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2019-09-26 20:35:22,620 [grpc-default-executor-1] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(293)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07->df1e4984-ea57-49fe-b366-a19f316192bd-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2019-09-26 20:35:22,625 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-26 20:35:22,626 [grpc-default-executor-1] INFO  impl.FollowerInfo (FollowerInfo.java:lambda$new$0(51)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07->df1e4984-ea57-49fe-b366-a19f316192bd: nextIndex: updateUnconditionally 1 -> 0
2019-09-26 20:35:22,627 [grpc-default-executor-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07-SegmentedRaftLogWorker close()
2019-09-26 20:35:22,626 [grpc-default-executor-3] INFO  impl.FollowerInfo (FollowerInfo.java:lambda$new$0(51)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-52FE9A5D9B07->d4e52575-d0bc-4eb0-97d4-5097dba1d6da: nextIndex: updateUnconditionally 1 -> 0
2019-09-26 20:35:22,651 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: remove  FOLLOWER d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07:t1, leader=5caead7e-358b-4710-835a-2d2cbee8bfb7, voted=5caead7e-358b-4710-835a-2d2cbee8bfb7, raftlog=d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null RUNNING
2019-09-26 20:35:22,651 [grpc-default-executor-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07: shutdown
2019-09-26 20:35:22,652 [grpc-default-executor-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-52FE9A5D9B07,id=d4e52575-d0bc-4eb0-97d4-5097dba1d6da
2019-09-26 20:35:22,652 [grpc-default-executor-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: shutdown FollowerState
2019-09-26 20:35:22,652 [grpc-default-executor-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07-StateMachineUpdater: set stopIndex = 0
2019-09-26 20:35:22,652 [Thread-228] INFO  impl.FollowerState (FollowerState.java:run(115)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-26 20:35:22,655 [grpc-default-executor-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07: closes. applyIndex: 0
2019-09-26 20:35:22,655 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-26 20:35:22,657 [grpc-default-executor-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-52FE9A5D9B07-SegmentedRaftLogWorker close()
2019-09-26 20:35:22,674 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - df1e4984-ea57-49fe-b366-a19f316192bd: remove  FOLLOWER df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07:t1, leader=5caead7e-358b-4710-835a-2d2cbee8bfb7, voted=5caead7e-358b-4710-835a-2d2cbee8bfb7, raftlog=df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952, 5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621, df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null RUNNING
2019-09-26 20:35:22,675 [grpc-default-executor-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07: shutdown
2019-09-26 20:35:22,675 [grpc-default-executor-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-52FE9A5D9B07,id=df1e4984-ea57-49fe-b366-a19f316192bd
2019-09-26 20:35:22,675 [grpc-default-executor-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - df1e4984-ea57-49fe-b366-a19f316192bd: shutdown FollowerState
2019-09-26 20:35:22,676 [grpc-default-executor-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07-StateMachineUpdater: set stopIndex = 0
2019-09-26 20:35:22,676 [Thread-229] INFO  impl.FollowerState (FollowerState.java:run(115)) - df1e4984-ea57-49fe-b366-a19f316192bd: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-26 20:35:22,678 [grpc-default-executor-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07: closes. applyIndex: 0
2019-09-26 20:35:22,678 [df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-26 20:35:22,679 [grpc-default-executor-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-52FE9A5D9B07-SegmentedRaftLogWorker close()
2019-09-26 20:35:22,688 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:removePipeline(108)) - Pipeline Pipeline[ Id: 7ae74b1f-d41a-4575-b405-52fe9a5d9b07, Nodes: 5caead7e-358b-4710-835a-2d2cbee8bfb7{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}d4e52575-d0bc-4eb0-97d4-5097dba1d6da{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}df1e4984-ea57-49fe-b366-a19f316192bd{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED] removed from db
2019-09-26 20:35:22,689 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: e103b091-44ec-4153-a353-04096f287bf8, Nodes: df1e4984-ea57-49fe-b366-a19f316192bd{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED]
2019-09-26 20:35:22,699 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - df1e4984-ea57-49fe-b366-a19f316192bd: remove    LEADER df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8:t1, leader=df1e4984-ea57-49fe-b366-a19f316192bd, voted=df1e4984-ea57-49fe-b366-a19f316192bd, raftlog=df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [df1e4984-ea57-49fe-b366-a19f316192bd:192.168.151.109:45194], old=null RUNNING
2019-09-26 20:35:22,700 [grpc-default-executor-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8: shutdown
2019-09-26 20:35:22,700 [grpc-default-executor-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-04096F287BF8,id=df1e4984-ea57-49fe-b366-a19f316192bd
2019-09-26 20:35:22,700 [grpc-default-executor-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - df1e4984-ea57-49fe-b366-a19f316192bd: shutdown LeaderState
2019-09-26 20:35:22,701 [grpc-default-executor-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - df1e4984-ea57-49fe-b366-a19f316192bd-PendingRequests: sendNotLeaderResponses
2019-09-26 20:35:22,701 [grpc-default-executor-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8-StateMachineUpdater: set stopIndex = 0
2019-09-26 20:35:22,702 [grpc-default-executor-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8: closes. applyIndex: 0
2019-09-26 20:35:22,703 [df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-26 20:35:22,704 [grpc-default-executor-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - df1e4984-ea57-49fe-b366-a19f316192bd@group-04096F287BF8-SegmentedRaftLogWorker close()
2019-09-26 20:35:22,712 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:removePipeline(108)) - Pipeline Pipeline[ Id: e103b091-44ec-4153-a353-04096f287bf8, Nodes: df1e4984-ea57-49fe-b366-a19f316192bd{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] removed from db
2019-09-26 20:35:22,714 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 0bf88db5-b565-4fd4-b518-12c2bdcf6179, Nodes: 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED]
2019-09-26 20:35:22,723 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: remove    LEADER 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179:t1, leader=8f7164d2-134a-4bb2-9a18-5a7fc6abc41e, voted=8f7164d2-134a-4bb2-9a18-5a7fc6abc41e, raftlog=8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e:192.168.151.109:43035], old=null RUNNING
2019-09-26 20:35:22,724 [grpc-default-executor-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179: shutdown
2019-09-26 20:35:22,724 [grpc-default-executor-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-12C2BDCF6179,id=8f7164d2-134a-4bb2-9a18-5a7fc6abc41e
2019-09-26 20:35:22,724 [grpc-default-executor-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: shutdown LeaderState
2019-09-26 20:35:22,724 [grpc-default-executor-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e-PendingRequests: sendNotLeaderResponses
2019-09-26 20:35:22,724 [grpc-default-executor-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179-StateMachineUpdater: set stopIndex = 0
2019-09-26 20:35:22,726 [grpc-default-executor-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179: closes. applyIndex: 0
2019-09-26 20:35:22,726 [8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-26 20:35:22,727 [grpc-default-executor-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e@group-12C2BDCF6179-SegmentedRaftLogWorker close()
2019-09-26 20:35:22,736 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:removePipeline(108)) - Pipeline Pipeline[ Id: 0bf88db5-b565-4fd4-b518-12c2bdcf6179, Nodes: 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] removed from db
2019-09-26 20:35:23,665 [main] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-26 20:35:23,665 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-26 20:35:23,666 [main] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - df1e4984-ea57-49fe-b366-a19f316192bd: close
2019-09-26 20:35:23,667 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: close
2019-09-26 20:35:23,669 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - df1e4984-ea57-49fe-b366-a19f316192bd: shutdown server with port 45194 now
2019-09-26 20:35:23,669 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: shutdown server with port 43035 now
2019-09-26 20:35:23,674 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - 8f7164d2-134a-4bb2-9a18-5a7fc6abc41e: shutdown server with port 43035 successfully
2019-09-26 20:35:23,674 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - df1e4984-ea57-49fe-b366-a19f316192bd: shutdown server with port 45194 successfully
2019-09-26 20:35:23,680 [refreshUsed-/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-2/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-26 20:35:23,687 [refreshUsed-/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-1/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-26 20:35:23,708 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-26 20:35:23,711 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-26 20:35:23,711 [main] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-26 20:35:23,715 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-26 20:35:23,718 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@12e0f1cb{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-26 20:35:23,719 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@59072e9d{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-26 20:35:23,720 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4a163575{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-26 20:35:23,720 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@58472096{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-26 20:35:23,720 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@36681447{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-26 20:35:23,721 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2f1d0bbc{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-26 20:35:23,721 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@77b919a3{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-26 20:35:23,722 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6cd64ee8{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-26 20:35:23,752 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-26 20:35:23,913 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-26 20:35:24,291 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:25,583 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode 5caead7e-358b-4710-835a-2d2cbee8bfb7{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=3e72e795-be68-4611-b8f9-2855985434a4]
2019-09-26 20:35:25,584 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 3e72e795-be68-4611-b8f9-2855985434a4, Nodes: 5caead7e-358b-4710-835a-2d2cbee8bfb7{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-26 20:35:25,584 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:finalizePipeline(123)) - Pipeline Pipeline[ Id: 3e72e795-be68-4611-b8f9-2855985434a4, Nodes: 5caead7e-358b-4710-835a-2d2cbee8bfb7{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] moved to CLOSED state
2019-09-26 20:35:25,783 [EventQueue-StaleNodeForStaleNodeHandler] INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode d4e52575-d0bc-4eb0-97d4-5097dba1d6da{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=df4205ee-2f8c-4168-bf02-cad149cf701a]
2019-09-26 20:35:25,784 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: df4205ee-2f8c-4168-bf02-cad149cf701a, Nodes: d4e52575-d0bc-4eb0-97d4-5097dba1d6da{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-26 20:35:25,784 [EventQueue-StaleNodeForStaleNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:finalizePipeline(123)) - Pipeline Pipeline[ Id: df4205ee-2f8c-4168-bf02-cad149cf701a, Nodes: d4e52575-d0bc-4eb0-97d4-5097dba1d6da{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] moved to CLOSED state
2019-09-26 20:35:26,292 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:27,587 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: 3e72e795-be68-4611-b8f9-2855985434a4, Nodes: 5caead7e-358b-4710-835a-2d2cbee8bfb7{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED]
2019-09-26 20:35:27,598 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: remove    LEADER 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4:t1, leader=5caead7e-358b-4710-835a-2d2cbee8bfb7, voted=5caead7e-358b-4710-835a-2d2cbee8bfb7, raftlog=5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [5caead7e-358b-4710-835a-2d2cbee8bfb7:192.168.151.109:46621], old=null RUNNING
2019-09-26 20:35:27,599 [grpc-default-executor-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4: shutdown
2019-09-26 20:35:27,599 [grpc-default-executor-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-2855985434A4,id=5caead7e-358b-4710-835a-2d2cbee8bfb7
2019-09-26 20:35:27,599 [grpc-default-executor-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: shutdown LeaderState
2019-09-26 20:35:27,600 [grpc-default-executor-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7-PendingRequests: sendNotLeaderResponses
2019-09-26 20:35:27,600 [grpc-default-executor-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4-StateMachineUpdater: set stopIndex = 0
2019-09-26 20:35:27,601 [grpc-default-executor-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4: closes. applyIndex: 0
2019-09-26 20:35:27,602 [5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-26 20:35:27,603 [grpc-default-executor-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7@group-2855985434A4-SegmentedRaftLogWorker close()
2019-09-26 20:35:27,615 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:removePipeline(108)) - Pipeline Pipeline[ Id: 3e72e795-be68-4611-b8f9-2855985434a4, Nodes: 5caead7e-358b-4710-835a-2d2cbee8bfb7{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] removed from db
2019-09-26 20:35:27,787 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(315)) - destroying pipeline:Pipeline[ Id: df4205ee-2f8c-4168-bf02-cad149cf701a, Nodes: d4e52575-d0bc-4eb0-97d4-5097dba1d6da{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED]
2019-09-26 20:35:27,799 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:remove(95)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: remove    LEADER d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A:t1, leader=d4e52575-d0bc-4eb0-97d4-5097dba1d6da, voted=d4e52575-d0bc-4eb0-97d4-5097dba1d6da, raftlog=d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [d4e52575-d0bc-4eb0-97d4-5097dba1d6da:192.168.151.109:43952], old=null RUNNING
2019-09-26 20:35:27,799 [grpc-default-executor-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A: shutdown
2019-09-26 20:35:27,799 [grpc-default-executor-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-CAD149CF701A,id=d4e52575-d0bc-4eb0-97d4-5097dba1d6da
2019-09-26 20:35:27,800 [grpc-default-executor-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: shutdown LeaderState
2019-09-26 20:35:27,800 [grpc-default-executor-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da-PendingRequests: sendNotLeaderResponses
2019-09-26 20:35:27,800 [grpc-default-executor-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A-StateMachineUpdater: set stopIndex = 0
2019-09-26 20:35:27,802 [grpc-default-executor-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A: closes. applyIndex: 0
2019-09-26 20:35:27,802 [d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-26 20:35:27,803 [grpc-default-executor-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da@group-CAD149CF701A-SegmentedRaftLogWorker close()
2019-09-26 20:35:27,812 [EventQueue-DeadNodeForDeadNodeHandler] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:removePipeline(108)) - Pipeline Pipeline[ Id: df4205ee-2f8c-4168-bf02-cad149cf701a, Nodes: d4e52575-d0bc-4eb0-97d4-5097dba1d6da{ip: 192.168.151.109, host: pr-hdds-2162-kpcbn-4093309294, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED] removed from db
2019-09-26 20:35:28,292 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-26 20:35:28,724 [main] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-26 20:35:28,724 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-26 20:35:28,725 [main] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: close
2019-09-26 20:35:28,725 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: close
2019-09-26 20:35:28,725 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: shutdown server with port 46621 now
2019-09-26 20:35:28,725 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: shutdown server with port 43952 now
2019-09-26 20:35:28,728 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - 5caead7e-358b-4710-835a-2d2cbee8bfb7: shutdown server with port 46621 successfully
2019-09-26 20:35:28,728 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - d4e52575-d0bc-4eb0-97d4-5097dba1d6da: shutdown server with port 43952 successfully
2019-09-26 20:35:28,730 [refreshUsed-/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-3/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-26 20:35:28,732 [refreshUsed-/workdir/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-9e8dacfe-69cb-419f-a095-97f0b9ebbb3d/datanode-0/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-26 20:35:29,052 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-26 20:35:29,061 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-26 20:35:29,063 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@a66e580{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-26 20:35:29,064 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@5b852b49{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-26 20:35:29,065 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7ff35a3f{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-26 20:35:29,066 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6d1dcdff{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-26 20:35:29,068 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-26 20:35:29,070 [main] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-26 20:35:29,071 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@437486cd{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-26 20:35:29,072 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@15b642b9{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-26 20:35:29,072 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5e7c141d{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-26 20:35:29,073 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@378cfecf{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-26 20:35:29,073 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopSCM(392)) - Stopping the StorageContainerManager
2019-09-26 20:35:29,074 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(808)) - Stopping Replication Manager Service.
2019-09-26 20:35:29,074 [main] INFO  container.ReplicationManager (ReplicationManager.java:stop(203)) - Stopping Replication Monitor Thread.
2019-09-26 20:35:29,074 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(815)) - Stopping Lease Manager of the command watchers
2019-09-26 20:35:29,074 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(822)) - Stopping datanode service RPC server
2019-09-26 20:35:29,075 [main] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:stop(367)) - Stopping the RPC server for DataNodes
2019-09-26 20:35:29,075 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 33697
2019-09-26 20:35:29,077 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-26 20:35:29,077 [IPC Server listener on 33697] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 33697
2019-09-26 20:35:29,116 [SCM Heartbeat Processing Thread - 0] WARN  node.NodeStateManager (NodeStateManager.java:scheduleNextHealthCheck(646)) - Current Thread is interrupted, shutting down HB processing thread for Node Manager.
2019-09-26 20:35:29,116 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(830)) - Stopping block service RPC server
2019-09-26 20:35:29,117 [main] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:stop(155)) - Stopping the RPC server for Block Protocol
2019-09-26 20:35:29,117 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 39985
2019-09-26 20:35:29,118 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(837)) - Stopping the StorageContainerLocationProtocol RPC server
2019-09-26 20:35:29,118 [IPC Server listener on 39985] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 39985
2019-09-26 20:35:29,119 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-26 20:35:29,119 [main] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:stop(159)) - Stopping the RPC server for Client Protocol
2019-09-26 20:35:29,120 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 46209
2019-09-26 20:35:29,122 [IPC Server listener on 46209] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 46209
2019-09-26 20:35:29,122 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(844)) - Stopping Storage Container Manager HTTP server.
2019-09-26 20:35:29,122 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-26 20:35:29,123 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@6928f576{/,null,UNAVAILABLE}{/scm}
2019-09-26 20:35:29,124 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@548e76f1{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-26 20:35:29,124 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3e821657{/static,file:///workdir/hadoop-ozone/integration-test/target/test-classes/webapps/static,UNAVAILABLE}
2019-09-26 20:35:29,125 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6b00f608{/logs,file:///workdir/hadoop-ozone/integration-test/target/log,UNAVAILABLE}
2019-09-26 20:35:29,125 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(855)) - Stopping Block Manager Service.
2019-09-26 20:35:29,126 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service SCMBlockDeletingService
2019-09-26 20:35:29,126 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service SCMBlockDeletingService
2019-09-26 20:35:29,127 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(877)) - Stopping SCM Event Queue.
2019-09-26 20:35:29,135 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping ratis metrics system...
2019-09-26 20:35:29,143 [prometheus] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:publishMetricsFromQueue(141)) - prometheus thread interrupted.
2019-09-26 20:35:29,143 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - ratis metrics system stopped.

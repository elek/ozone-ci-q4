2019-09-28 05:55:34,949 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-28 05:55:35,059 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-28 05:55:35,063 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-28 05:55:35,083 [main] INFO  util.log (Log.java:initialized(192)) - Logging initialized @924ms
2019-09-28 05:55:35,189 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: deletedBlocks
2019-09-28 05:55:35,190 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:deletedBlocks
2019-09-28 05:55:35,190 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: validCerts
2019-09-28 05:55:35,190 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:validCerts
2019-09-28 05:55:35,191 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: revokedCerts
2019-09-28 05:55:35,191 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:revokedCerts
2019-09-28 05:55:35,204 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: default
2019-09-28 05:55:35,204 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(167)) - Using default column profile:DBProfile.DISK for Table:default
2019-09-28 05:55:35,206 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:getDbProfile(198)) - Using default options. DBProfile.DISK
2019-09-28 05:55:35,534 [main] INFO  net.NodeSchemaLoader (NodeSchemaLoader.java:loadSchemaFromFile(125)) - Loading file from sun.misc.CompoundEnumeration@59474f18
2019-09-28 05:55:35,536 [main] INFO  net.NodeSchemaLoader (NodeSchemaLoader.java:loadSchema(171)) - Loading network topology layer schema file
2019-09-28 05:55:35,613 [main] WARN  server.ServerUtils (ServerUtils.java:sanitizeUserArgs(70)) - ozone.scm.stale.node.interval value = 300000 is larger than max = 100000 based on the key value of ozone.scm.heartbeat.thread.interval, reset to the max value 100000.
2019-09-28 05:55:35,615 [main] WARN  server.ServerUtils (ServerUtils.java:sanitizeUserArgs(70)) - ozone.scm.stale.node.interval value = 300000 is larger than max = 100000 based on the key value of ozone.scm.heartbeat.thread.interval, reset to the max value 100000.
2019-09-28 05:55:35,619 [main] INFO  node.SCMNodeManager (SCMNodeManager.java:<init>(114)) - Entering startup safe mode.
2019-09-28 05:55:35,701 [main] INFO  algorithms.ContainerPlacementPolicyFactory (ContainerPlacementPolicyFactory.java:getPolicy(57)) - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
2019-09-28 05:55:35,717 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-28 05:55:35,810 [main] INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:initializePipelineState(132)) - No pipeline exists in current db
2019-09-28 05:55:35,813 [main] WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(145)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-28 05:55:35,946 [main] WARN  events.EventQueue (EventQueue.java:fireEvent(183)) - No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='SafeModeStatus'}
ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
2019-09-28 05:55:36,333 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-28 05:55:36,362 [Socket Reader #1 for port 40752] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 40752
2019-09-28 05:55:36,512 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-28 05:55:36,514 [Socket Reader #1 for port 41337] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 41337
2019-09-28 05:55:36,523 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-28 05:55:36,524 [Socket Reader #1 for port 37564] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 37564
2019-09-28 05:55:36,549 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for scm at: http://0.0.0.0:0
2019-09-28 05:55:36,714 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-28 05:55:36,732 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-28 05:55:36,747 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-28 05:55:36,751 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
2019-09-28 05:55:36,752 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-28 05:55:36,752 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-28 05:55:36,787 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(770)) - StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:37564
2019-09-28 05:55:36,845 [main] WARN  impl.MetricsConfig (MetricsConfig.java:loadFirst(134)) - Cannot locate configuration: tried hadoop-metrics2-storagecontainermanager.properties,hadoop-metrics2.properties
2019-09-28 05:55:36,858 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 10 second(s).
2019-09-28 05:55:36,859 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - StorageContainerManager metrics system started
2019-09-28 05:55:37,092 [main] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:start(150)) - RPC server for Client  is listening at /0.0.0.0:37564
2019-09-28 05:55:37,093 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-28 05:55:37,093 [IPC Server listener on 37564] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 37564: starting
2019-09-28 05:55:37,095 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(780)) - ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:41337
2019-09-28 05:55:37,097 [main] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:start(146)) - RPC server for Block Protocol is listening at /0.0.0.0:41337
2019-09-28 05:55:37,097 [IPC Server listener on 41337] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 41337: starting
2019-09-28 05:55:37,097 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-28 05:55:37,100 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(784)) - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:40752
2019-09-28 05:55:37,100 [main] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:start(191)) - RPC server for DataNodes is listening at /0.0.0.0:40752
2019-09-28 05:55:37,100 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-28 05:55:37,101 [IPC Server listener on 40752] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 40752: starting
2019-09-28 05:55:37,104 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 43206
2019-09-28 05:55:37,106 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-28 05:55:37,140 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@44e3a2b2{/logs,file:///workdir/hadoop-ozone/tools/target/log,AVAILABLE}
2019-09-28 05:55:37,140 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@44040454{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-server-scm/0.5.0-SNAPSHOT/hadoop-hdds-server-scm-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-28 05:55:37,215 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@544630b7{/,file:///tmp/jetty-0.0.0.0-43206-scm-_-any-6863847184028906883.dir/webapp/,AVAILABLE}{/scm}
2019-09-28 05:55:37,219 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@514eedd8{HTTP/1.1,[http/1.1]}{0.0.0.0:43206}
2019-09-28 05:55:37,220 [main] INFO  server.Server (Server.java:doStart(419)) - Started @3061ms
2019-09-28 05:55:37,222 [main] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:start(204)) - Sink prometheus started
2019-09-28 05:55:37,222 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:registerSink(301)) - Registered sink prometheus
2019-09-28 05:55:37,224 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of SCM is listening at http://0.0.0.0:43206
2019-09-28 05:55:37,231 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@790174f2] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-28 05:55:37,235 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-28 05:55:37,348 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:loadOMHAConfig(190)) - Configuration either no ozone.om.address set. Falling back to the default OM address /127.0.0.1:0
2019-09-28 05:55:37,348 [main] INFO  ha.OMHANodeDetails (OMHANodeDetails.java:getOMNodeDetails(220)) - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
2019-09-28 05:55:37,350 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-28 05:55:37,350 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-28 05:55:38,106 [main] WARN  server.ServerUtils (ServerUtils.java:getDBPath(222)) - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2019-09-28 05:55:38,115 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: userTable
2019-09-28 05:55:38,115 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:userTable
2019-09-28 05:55:38,115 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: volumeTable
2019-09-28 05:55:38,115 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:volumeTable
2019-09-28 05:55:38,116 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: bucketTable
2019-09-28 05:55:38,116 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:bucketTable
2019-09-28 05:55:38,116 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: keyTable
2019-09-28 05:55:38,116 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:keyTable
2019-09-28 05:55:38,117 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: deletedTable
2019-09-28 05:55:38,117 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:deletedTable
2019-09-28 05:55:38,117 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: openKeyTable
2019-09-28 05:55:38,117 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:openKeyTable
2019-09-28 05:55:38,118 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: s3Table
2019-09-28 05:55:38,118 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:s3Table
2019-09-28 05:55:38,118 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: multipartInfoTable
2019-09-28 05:55:38,118 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:multipartInfoTable
2019-09-28 05:55:38,119 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: dTokenTable
2019-09-28 05:55:38,119 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:dTokenTable
2019-09-28 05:55:38,119 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: s3SecretTable
2019-09-28 05:55:38,120 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:s3SecretTable
2019-09-28 05:55:38,120 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: prefixTable
2019-09-28 05:55:38,120 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(161)) - Using default column profile:DBProfile.DISK for Table:prefixTable
2019-09-28 05:55:38,120 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:addTable(110)) - using custom profile for table: default
2019-09-28 05:55:38,121 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:processTables(167)) - Using default column profile:DBProfile.DISK for Table:default
2019-09-28 05:55:38,121 [main] INFO  db.DBStoreBuilder (DBStoreBuilder.java:getDbProfile(198)) - Using default options. DBProfile.DISK
2019-09-28 05:55:38,799 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
2019-09-28 05:55:38,800 [Socket Reader #1 for port 43113] INFO  ipc.Server (Server.java:run(1074)) - Starting Socket Reader #1 for port 43113
2019-09-28 05:55:38,823 [main] INFO  om.OzoneManager (OzoneManager.java:start(1069)) - OzoneManager RPC server is listening at localhost/127.0.0.1:43113
2019-09-28 05:55:38,824 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - OzoneManager metrics system started (again)
2019-09-28 05:55:38,825 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1314)) - IPC Server Responder: starting
2019-09-28 05:55:38,826 [IPC Server listener on 43113] INFO  ipc.Server (Server.java:run(1153)) - IPC Server listener on 43113: starting
2019-09-28 05:55:38,832 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for ozoneManager at: http://0.0.0.0:0
2019-09-28 05:55:38,835 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-28 05:55:38,836 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-28 05:55:38,838 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-28 05:55:38,839 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
2019-09-28 05:55:38,840 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-28 05:55:38,840 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-28 05:55:38,845 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 38609
2019-09-28 05:55:38,847 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-28 05:55:38,852 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@22d1886d{/logs,file:///workdir/hadoop-ozone/tools/target/log,AVAILABLE}
2019-09-28 05:55:38,853 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1cbb3d3b{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-ozone-ozone-manager/0.5.0-SNAPSHOT/hadoop-ozone-ozone-manager-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-28 05:55:38,920 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@3e850122{/,file:///tmp/jetty-0.0.0.0-38609-ozoneManager-_-any-1388182564664697589.dir/webapp/,AVAILABLE}{/ozoneManager}
2019-09-28 05:55:38,923 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@27fde870{HTTP/1.1,[http/1.1]}{0.0.0.0:38609}
2019-09-28 05:55:38,923 [main] INFO  server.Server (Server.java:doStart(419)) - Started @4765ms
2019-09-28 05:55:38,924 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-28 05:55:38,925 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of OZONEMANAGER is listening at http://0.0.0.0:38609
2019-09-28 05:55:39,287 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-28 05:55:39,338 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-8g7kw-3348123894 ip:192.168.151.82
2019-09-28 05:55:39,376 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-0/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-28 05:55:39,379 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-0/data/containers/hdds to VolumeSet
2019-09-28 05:55:39,382 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@acf859d
2019-09-28 05:55:39,400 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@acf859d
2019-09-28 05:55:39,529 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-28 05:55:39,601 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-28 05:55:39,606 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-28 05:55:39,607 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-28 05:55:39,608 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:39,609 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-28 05:55:39,610 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-28 05:55:39,800 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-0/data/ratis] (custom)
2019-09-28 05:55:39,874 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-28 05:55:39,876 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-28 05:55:39,877 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-28 05:55:39,879 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-28 05:55:39,880 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-28 05:55:39,881 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-28 05:55:39,881 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-28 05:55:39,882 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 45811
2019-09-28 05:55:39,882 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-28 05:55:39,885 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@895416d{/logs,file:///workdir/hadoop-ozone/tools/target/log,AVAILABLE}
2019-09-28 05:55:39,886 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@80bfdc6{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-28 05:55:39,926 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@625d9132{/,file:///tmp/jetty-0.0.0.0-45811-hddsDatanode-_-any-8806582937942236978.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-28 05:55:39,926 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@77774571{HTTP/1.1,[http/1.1]}{0.0.0.0:45811}
2019-09-28 05:55:39,928 [main] INFO  server.Server (Server.java:doStart(419)) - Started @5770ms
2019-09-28 05:55:39,928 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-28 05:55:39,929 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:45811
2019-09-28 05:55:39,930 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-28 05:55:39,933 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-8g7kw-3348123894 ip:192.168.151.82
2019-09-28 05:55:39,937 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@bbfabd0] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-28 05:55:39,941 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-28 05:55:39,942 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/containers/hdds to VolumeSet
2019-09-28 05:55:39,942 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@79b84841
2019-09-28 05:55:39,943 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@79b84841
2019-09-28 05:55:39,958 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-28 05:55:39,958 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-28 05:55:39,959 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-28 05:55:39,959 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-28 05:55:39,959 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:39,959 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-28 05:55:39,960 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-28 05:55:39,960 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/ratis] (custom)
2019-09-28 05:55:39,962 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-28 05:55:39,963 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-28 05:55:39,964 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-28 05:55:39,966 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-28 05:55:39,966 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-28 05:55:39,966 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-28 05:55:39,966 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-28 05:55:39,967 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 38750
2019-09-28 05:55:39,967 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-28 05:55:39,971 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@4da6d664{/logs,file:///workdir/hadoop-ozone/tools/target/log,AVAILABLE}
2019-09-28 05:55:39,972 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@30b131b2{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-28 05:55:40,000 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@77b919a3{/,file:///tmp/jetty-0.0.0.0-38750-hddsDatanode-_-any-1324371721828664727.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-28 05:55:40,001 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@5624657a{HTTP/1.1,[http/1.1]}{0.0.0.0:38750}
2019-09-28 05:55:40,002 [main] INFO  server.Server (Server.java:doStart(419)) - Started @5844ms
2019-09-28 05:55:40,003 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-28 05:55:40,003 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:38750
2019-09-28 05:55:40,004 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-28 05:55:40,007 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-8g7kw-3348123894 ip:192.168.151.82
2019-09-28 05:55:40,007 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@41f345b9] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-28 05:55:40,015 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-28 05:55:40,016 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/containers/hdds to VolumeSet
2019-09-28 05:55:40,016 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@733aa9d8
2019-09-28 05:55:40,016 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@733aa9d8
2019-09-28 05:55:40,032 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-28 05:55:40,032 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-28 05:55:40,033 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-28 05:55:40,033 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-28 05:55:40,033 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:40,033 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-28 05:55:40,034 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-28 05:55:40,034 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/ratis] (custom)
2019-09-28 05:55:40,036 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-28 05:55:40,038 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-28 05:55:40,038 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-28 05:55:40,040 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-28 05:55:40,040 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-28 05:55:40,040 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-28 05:55:40,040 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-28 05:55:40,042 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 33470
2019-09-28 05:55:40,043 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-28 05:55:40,045 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5a6fa56e{/logs,file:///workdir/hadoop-ozone/tools/target/log,AVAILABLE}
2019-09-28 05:55:40,045 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@118ffcfd{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-28 05:55:40,046 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-0/meta/datanode.id
2019-09-28 05:55:40,052 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/meta/datanode.id
2019-09-28 05:55:40,077 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@378cfecf{/,file:///tmp/jetty-0.0.0.0-33470-hddsDatanode-_-any-4818080113542472699.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-28 05:55:40,078 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@97d0c06{HTTP/1.1,[http/1.1]}{0.0.0.0:33470}
2019-09-28 05:55:40,079 [main] INFO  server.Server (Server.java:doStart(419)) - Started @5920ms
2019-09-28 05:55:40,080 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-28 05:55:40,081 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:33470
2019-09-28 05:55:40,081 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-28 05:55:40,084 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-8g7kw-3348123894 ip:192.168.151.82
2019-09-28 05:55:40,084 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2622faa3] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-28 05:55:40,088 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/meta/datanode.id
2019-09-28 05:55:40,093 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-28 05:55:40,093 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/containers/hdds to VolumeSet
2019-09-28 05:55:40,093 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@4e1a46fb
2019-09-28 05:55:40,094 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@4e1a46fb
2019-09-28 05:55:40,109 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-28 05:55:40,109 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-28 05:55:40,109 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-28 05:55:40,109 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-28 05:55:40,110 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:40,110 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-28 05:55:40,110 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-28 05:55:40,111 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/ratis] (custom)
2019-09-28 05:55:40,112 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-28 05:55:40,113 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-28 05:55:40,114 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-28 05:55:40,115 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-28 05:55:40,115 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-28 05:55:40,116 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-28 05:55:40,116 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-28 05:55:40,116 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 43627
2019-09-28 05:55:40,117 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-28 05:55:40,120 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7fb29ca9{/logs,file:///workdir/hadoop-ozone/tools/target/log,AVAILABLE}
2019-09-28 05:55:40,121 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@350d3f4d{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-28 05:55:40,246 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@67e13bd0{/,file:///tmp/jetty-0.0.0.0-43627-hddsDatanode-_-any-5426558421998862159.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-28 05:55:40,248 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@50fb33a{HTTP/1.1,[http/1.1]}{0.0.0.0:43627}
2019-09-28 05:55:40,249 [main] INFO  server.Server (Server.java:doStart(419)) - Started @6090ms
2019-09-28 05:55:40,249 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-28 05:55:40,251 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:43627
2019-09-28 05:55:40,251 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - HddsDatanode metrics system started (again)
2019-09-28 05:55:40,256 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2e19b60d] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-28 05:55:40,257 [main] INFO  ozone.HddsDatanodeService (HddsDatanodeService.java:start(184)) - HddsDatanodeService host:pr-hdds-2162-8g7kw-3348123894 ip:192.168.151.82
2019-09-28 05:55:40,261 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/meta/datanode.id
2019-09-28 05:55:40,269 [main] INFO  volume.HddsVolume (HddsVolume.java:<init>(176)) - Creating Volume: /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-4/data/containers/hdds of  storage type : DISK and capacity : 1482555576320
2019-09-28 05:55:40,269 [main] INFO  volume.VolumeSet (VolumeSet.java:initializeVolumeSet(170)) - Added Volume : /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-4/data/containers/hdds to VolumeSet
2019-09-28 05:55:40,270 [main] INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(139)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@75d982d3
2019-09-28 05:55:40,271 [main] INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(202)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@75d982d3
2019-09-28 05:55:40,293 [main] INFO  impl.RaftServerProxy (ConfUtils.java:logGet(43)) - raft.rpc.type = GRPC (default)
2019-09-28 05:55:40,293 [main] INFO  grpc.GrpcFactory (GrpcFactory.java:checkPooledByteBufAllocatorUseCacheForAllThreads(45)) - PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
2019-09-28 05:55:40,294 [main] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.port = 0 (default)
2019-09-28 05:55:40,294 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.message.size.max = 33570816 (custom)
2019-09-28 05:55:40,294 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:40,295 [main] INFO  server.GrpcService (ConfUtils.java:logGet(43)) - raft.grpc.flow.control.window = 1MB (=1048576) (default)
2019-09-28 05:55:40,295 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-28 05:55:40,296 [main] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-4/data/ratis] (custom)
2019-09-28 05:55:40,298 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1641)) - Starting Web-server for hddsDatanode at: http://0.0.0.0:0
2019-09-28 05:55:40,300 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-28 05:55:40,301 [main] WARN  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(97)) - Jetty request log can only be enabled using Log4j
2019-09-28 05:55:40,303 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(975)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-28 05:55:40,304 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(948)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
2019-09-28 05:55:40,304 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-28 05:55:40,305 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(958)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-28 05:55:40,306 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1191)) - Jetty bound to port 45843
2019-09-28 05:55:40,306 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732
2019-09-28 05:55:40,309 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5ea4300e{/logs,file:///workdir/hadoop-ozone/tools/target/log,AVAILABLE}
2019-09-28 05:55:40,310 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@76ad6715{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2019-09-28 05:55:40,347 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@7e53339{/,file:///tmp/jetty-0.0.0.0-45843-hddsDatanode-_-any-1220952393977587864.dir/webapp/,AVAILABLE}{/hddsDatanode}
2019-09-28 05:55:40,348 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@58fef7f7{HTTP/1.1,[http/1.1]}{0.0.0.0:45843}
2019-09-28 05:55:40,349 [main] INFO  server.Server (Server.java:doStart(419)) - Started @6190ms
2019-09-28 05:55:40,349 [main] WARN  impl.MetricsSystemImpl (MetricsSystemImpl.java:register(276)) - Sink prometheus already exists!
2019-09-28 05:55:40,350 [main] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(215)) - HTTP server of HDDSDATANODE is listening at http://0.0.0.0:45843
2019-09-28 05:55:40,352 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 5 DN Heartbeats.
2019-09-28 05:55:40,353 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5ab2435a] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2019-09-28 05:55:40,357 [Datanode State Machine Thread - 0] INFO  datanode.InitDatanodeState (InitDatanodeState.java:persistContainerDatanodeDetails(140)) - DatanodeDetails is persisted to /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-4/meta/datanode.id
2019-09-28 05:55:41,353 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 5 DN Heartbeats.
2019-09-28 05:55:41,994 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-28 05:55:41,997 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-28 05:55:41,998 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis 1db3c405-2e55-4b17-b9d1-8ec21761eee8 at port 0
2019-09-28 05:55:42,025 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8: start RPC server
2019-09-28 05:55:42,026 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-28 05:55:42,031 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-28 05:55:42,034 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis 86ebeb77-e9e6-4e56-959d-17a96123c217 at port 0
2019-09-28 05:55:42,043 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: start RPC server
2019-09-28 05:55:42,101 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-28 05:55:42,103 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-28 05:55:42,103 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis bc198d42-aabb-46be-826b-770a90b8899e at port 0
2019-09-28 05:55:42,111 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - bc198d42-aabb-46be-826b-770a90b8899e: start RPC server
2019-09-28 05:55:42,189 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: GrpcService started, listening on 0.0.0.0/0.0.0.0:44635
2019-09-28 05:55:42,189 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - bc198d42-aabb-46be-826b-770a90b8899e: GrpcService started, listening on 0.0.0.0/0.0.0.0:37666
2019-09-28 05:55:42,189 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8: GrpcService started, listening on 0.0.0.0/0.0.0.0:33114
2019-09-28 05:55:42,190 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis bc198d42-aabb-46be-826b-770a90b8899e is started using port 37666
2019-09-28 05:55:42,190 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis 86ebeb77-e9e6-4e56-959d-17a96123c217 is started using port 44635
2019-09-28 05:55:42,190 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis 1db3c405-2e55-4b17-b9d1-8ec21761eee8 is started using port 33114
2019-09-28 05:55:42,197 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc 1db3c405-2e55-4b17-b9d1-8ec21761eee8 is started using port 35120
2019-09-28 05:55:42,197 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc 86ebeb77-e9e6-4e56-959d-17a96123c217 is started using port 45475
2019-09-28 05:55:42,197 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc bc198d42-aabb-46be-826b-770a90b8899e is started using port 36037
2019-09-28 05:55:42,273 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-28 05:55:42,275 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-28 05:55:42,275 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis 6f360f6f-30d2-485d-bdb1-20146522ae70 at port 0
2019-09-28 05:55:42,283 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: start RPC server
2019-09-28 05:55:42,287 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: GrpcService started, listening on 0.0.0.0/0.0.0.0:38933
2019-09-28 05:55:42,287 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis 6f360f6f-30d2-485d-bdb1-20146522ae70 is started using port 38933
2019-09-28 05:55:42,290 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc 6f360f6f-30d2-485d-bdb1-20146522ae70 is started using port 46813
2019-09-28 05:55:42,353 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 5 DN Heartbeats.
2019-09-28 05:55:42,371 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:start(217)) - Attempting to start container services.
2019-09-28 05:55:42,377 [Datanode State Machine Thread - 0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:startContainerScrub(179)) - Background container scanner has been disabled.
2019-09-28 05:55:42,377 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(411)) - Starting XceiverServerRatis 301de80a-213b-4abd-bdb2-9b127524bed9 at port 0
2019-09-28 05:55:42,386 [Datanode State Machine Thread - 0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$start$3(299)) - 301de80a-213b-4abd-bdb2-9b127524bed9: start RPC server
2019-09-28 05:55:42,390 [Datanode State Machine Thread - 0] INFO  server.GrpcService (GrpcService.java:startImpl(158)) - 301de80a-213b-4abd-bdb2-9b127524bed9: GrpcService started, listening on 0.0.0.0/0.0.0.0:38999
2019-09-28 05:55:42,390 [Datanode State Machine Thread - 0] INFO  ratis.XceiverServerRatis (XceiverServerRatis.java:start(421)) - XceiverServerRatis 301de80a-213b-4abd-bdb2-9b127524bed9 is started using port 38999
2019-09-28 05:55:42,393 [Datanode State Machine Thread - 0] INFO  server.XceiverServerGrpc (XceiverServerGrpc.java:start(149)) - XceiverServerGrpc 301de80a-213b-4abd-bdb2-9b127524bed9 is started using port 40505
2019-09-28 05:55:43,354 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 0 of 5 DN Heartbeats.
2019-09-28 05:55:43,980 [IPC Server handler 15 on 40752] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/1db3c405-2e55-4b17-b9d1-8ec21761eee8
2019-09-28 05:55:43,981 [IPC Server handler 15 on 40752] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : 1db3c405-2e55-4b17-b9d1-8ec21761eee8{ip: 192.168.151.82, host: pr-hdds-2162-8g7kw-3348123894, networkLocation: /default-rack, certSerialId: null}
2019-09-28 05:55:43,986 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO  safemode.SCMSafeModeManager (DataNodeSafeModeRule.java:process(71)) - SCM in safe mode. 1 DataNodes registered, 1 required.
2019-09-28 05:55:43,986 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:validateSafeModeExitRules(177)) - ScmSafeModeManager, all rules are successfully validated
2019-09-28 05:55:43,986 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO  safemode.SCMSafeModeManager (SCMSafeModeManager.java:exitSafeMode(193)) - SCM exiting safe mode.
2019-09-28 05:55:44,009 [IPC Server handler 14 on 40752] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/86ebeb77-e9e6-4e56-959d-17a96123c217
2019-09-28 05:55:44,010 [IPC Server handler 14 on 40752] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : 86ebeb77-e9e6-4e56-959d-17a96123c217{ip: 192.168.151.82, host: pr-hdds-2162-8g7kw-3348123894, networkLocation: /default-rack, certSerialId: null}
2019-09-28 05:55:44,089 [IPC Server handler 12 on 40752] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/bc198d42-aabb-46be-826b-770a90b8899e
2019-09-28 05:55:44,090 [IPC Server handler 12 on 40752] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : bc198d42-aabb-46be-826b-770a90b8899e{ip: 192.168.151.82, host: pr-hdds-2162-8g7kw-3348123894, networkLocation: /default-rack, certSerialId: null}
2019-09-28 05:55:44,261 [IPC Server handler 19 on 40752] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/6f360f6f-30d2-485d-bdb1-20146522ae70
2019-09-28 05:55:44,262 [IPC Server handler 19 on 40752] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : 6f360f6f-30d2-485d-bdb1-20146522ae70{ip: 192.168.151.82, host: pr-hdds-2162-8g7kw-3348123894, networkLocation: /default-rack, certSerialId: null}
2019-09-28 05:55:44,354 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Waiting for cluster to be ready. Got 4 of 5 DN Heartbeats.
2019-09-28 05:55:44,358 [IPC Server handler 15 on 40752] INFO  net.NetworkTopology (NetworkTopologyImpl.java:add(111)) - Added a new node: /default-rack/301de80a-213b-4abd-bdb2-9b127524bed9
2019-09-28 05:55:44,358 [IPC Server handler 15 on 40752] INFO  node.SCMNodeManager (SCMNodeManager.java:register(266)) - Registered Data node : 301de80a-213b-4abd-bdb2-9b127524bed9{ip: 192.168.151.82, host: pr-hdds-2162-8g7kw-3348123894, networkLocation: /default-rack, certSerialId: null}
2019-09-28 05:55:44,502 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8: addNew group-E59B61FA56EE:[1db3c405-2e55-4b17-b9d1-8ec21761eee8:192.168.151.82:33114] returns group-E59B61FA56EE:java.util.concurrent.CompletableFuture@18949cde[Not completed]
2019-09-28 05:55:44,519 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8: new RaftServerImpl for group-E59B61FA56EE:[1db3c405-2e55-4b17-b9d1-8ec21761eee8:192.168.151.82:33114] with ContainerStateMachine:uninitialized
2019-09-28 05:55:44,521 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-28 05:55:44,523 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-28 05:55:44,523 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-28 05:55:44,524 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-28 05:55:44,525 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-28 05:55:44,533 [pool-27-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE: ConfigurationManager, init=-1: [1db3c405-2e55-4b17-b9d1-8ec21761eee8:192.168.151.82:33114], old=null, confs=<EMPTY_MAP>
2019-09-28 05:55:44,534 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-0/data/ratis] (custom)
2019-09-28 05:55:44,541 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-0/data/ratis/05001a20-6613-4f60-ad80-e59b61fa56ee does not exist. Creating ...
2019-09-28 05:55:44,587 [pool-27-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-0/data/ratis/05001a20-6613-4f60-ad80-e59b61fa56ee/in_use.lock acquired by nodename 21794@pr-hdds-2162-8g7kw-3348123894
2019-09-28 05:55:44,603 [pool-27-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-0/data/ratis/05001a20-6613-4f60-ad80-e59b61fa56ee has been successfully formatted.
2019-09-28 05:55:44,606 [pool-27-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-E59B61FA56EE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-28 05:55:44,606 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-28 05:55:44,608 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-28 05:55:44,614 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-28 05:55:44,615 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:44,617 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:44,622 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-28 05:55:44,627 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-0/data/ratis/05001a20-6613-4f60-ad80-e59b61fa56ee
2019-09-28 05:55:44,629 [pool-27-thread-1] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - ratis metrics system started (again)
2019-09-28 05:55:44,635 [pool-27-thread-1] INFO  metrics.MetricRegistries (MetricRegistriesLoader.java:load(64)) - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
2019-09-28 05:55:44,685 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-28 05:55:44,686 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-28 05:55:44,690 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:44,690 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-28 05:55:44,691 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-28 05:55:44,692 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-28 05:55:44,693 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-28 05:55:44,693 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-28 05:55:44,694 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-28 05:55:44,707 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-28 05:55:44,712 [pool-27-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-28 05:55:44,717 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-28 05:55:44,718 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-28 05:55:44,719 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-28 05:55:44,719 [pool-27-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-28 05:55:44,747 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE: start as a follower, conf=-1: [1db3c405-2e55-4b17-b9d1-8ec21761eee8:192.168.151.82:33114], old=null
2019-09-28 05:55:44,748 [pool-27-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-28 05:55:44,749 [pool-27-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8: start FollowerState
2019-09-28 05:55:44,751 [pool-27-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E59B61FA56EE,id=1db3c405-2e55-4b17-b9d1-8ec21761eee8
2019-09-28 05:55:44,807 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 05001a20-6613-4f60-ad80-e59b61fa56ee, Nodes: 1db3c405-2e55-4b17-b9d1-8ec21761eee8{ip: 192.168.151.82, host: pr-hdds-2162-8g7kw-3348123894, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-28 05:55:44,827 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: addNew group-36418E16FEF6:[6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933] returns group-36418E16FEF6:java.util.concurrent.CompletableFuture@46921b7a[Not completed]
2019-09-28 05:55:44,866 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: new RaftServerImpl for group-36418E16FEF6:[6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933] with ContainerStateMachine:uninitialized
2019-09-28 05:55:44,867 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-28 05:55:44,867 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-28 05:55:44,867 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-28 05:55:44,867 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-28 05:55:44,867 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-28 05:55:44,868 [pool-57-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6: ConfigurationManager, init=-1: [6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933], old=null, confs=<EMPTY_MAP>
2019-09-28 05:55:44,868 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/ratis] (custom)
2019-09-28 05:55:44,868 [pool-57-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/ratis/098a7288-c4bf-417c-8332-36418e16fef6 does not exist. Creating ...
2019-09-28 05:55:44,882 [pool-57-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/ratis/098a7288-c4bf-417c-8332-36418e16fef6/in_use.lock acquired by nodename 21794@pr-hdds-2162-8g7kw-3348123894
2019-09-28 05:55:44,896 [pool-57-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/ratis/098a7288-c4bf-417c-8332-36418e16fef6 has been successfully formatted.
2019-09-28 05:55:44,897 [pool-57-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-36418E16FEF6: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-28 05:55:44,898 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-28 05:55:44,899 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-28 05:55:44,899 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-28 05:55:44,899 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:44,899 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:44,899 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-28 05:55:44,900 [pool-57-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/ratis/098a7288-c4bf-417c-8332-36418e16fef6
2019-09-28 05:55:44,905 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-28 05:55:44,905 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-28 05:55:44,905 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:44,906 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-28 05:55:44,906 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-28 05:55:44,906 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-28 05:55:44,906 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-28 05:55:44,906 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-28 05:55:44,906 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-28 05:55:44,907 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-28 05:55:44,907 [pool-57-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-28 05:55:44,907 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-28 05:55:44,907 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-28 05:55:44,907 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-28 05:55:44,908 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-28 05:55:44,912 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6: start as a follower, conf=-1: [6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933], old=null
2019-09-28 05:55:44,912 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-28 05:55:44,912 [pool-57-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: start FollowerState
2019-09-28 05:55:44,913 [pool-57-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-36418E16FEF6,id=6f360f6f-30d2-485d-bdb1-20146522ae70
2019-09-28 05:55:44,926 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 098a7288-c4bf-417c-8332-36418e16fef6, Nodes: 6f360f6f-30d2-485d-bdb1-20146522ae70{ip: 192.168.151.82, host: pr-hdds-2162-8g7kw-3348123894, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-28 05:55:44,942 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - bc198d42-aabb-46be-826b-770a90b8899e: addNew group-707AEF46CAE3:[bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666] returns group-707AEF46CAE3:java.util.concurrent.CompletableFuture@2535c7d0[Not completed]
2019-09-28 05:55:44,966 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - bc198d42-aabb-46be-826b-770a90b8899e: new RaftServerImpl for group-707AEF46CAE3:[bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666] with ContainerStateMachine:uninitialized
2019-09-28 05:55:44,967 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-28 05:55:44,967 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-28 05:55:44,967 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-28 05:55:44,967 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-28 05:55:44,967 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-28 05:55:44,968 [pool-47-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3: ConfigurationManager, init=-1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666], old=null, confs=<EMPTY_MAP>
2019-09-28 05:55:44,968 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/ratis] (custom)
2019-09-28 05:55:44,968 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/ratis/143d12b9-c00c-4eb4-ae18-707aef46cae3 does not exist. Creating ...
2019-09-28 05:55:44,982 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/ratis/143d12b9-c00c-4eb4-ae18-707aef46cae3/in_use.lock acquired by nodename 21794@pr-hdds-2162-8g7kw-3348123894
2019-09-28 05:55:44,996 [pool-47-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/ratis/143d12b9-c00c-4eb4-ae18-707aef46cae3 has been successfully formatted.
2019-09-28 05:55:44,996 [pool-47-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-707AEF46CAE3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-28 05:55:44,996 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-28 05:55:44,996 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-28 05:55:44,997 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-28 05:55:44,997 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:44,997 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:44,997 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-28 05:55:44,997 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/ratis/143d12b9-c00c-4eb4-ae18-707aef46cae3
2019-09-28 05:55:45,023 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-28 05:55:45,023 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-28 05:55:45,023 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:45,023 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-28 05:55:45,024 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-28 05:55:45,024 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-28 05:55:45,024 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-28 05:55:45,024 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-28 05:55:45,024 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-28 05:55:45,024 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-28 05:55:45,025 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-28 05:55:45,025 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-28 05:55:45,025 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-28 05:55:45,025 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-28 05:55:45,025 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-28 05:55:45,029 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3: start as a follower, conf=-1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666], old=null
2019-09-28 05:55:45,029 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-28 05:55:45,030 [pool-47-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - bc198d42-aabb-46be-826b-770a90b8899e: start FollowerState
2019-09-28 05:55:45,030 [pool-47-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-707AEF46CAE3,id=bc198d42-aabb-46be-826b-770a90b8899e
2019-09-28 05:55:45,041 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 143d12b9-c00c-4eb4-ae18-707aef46cae3, Nodes: bc198d42-aabb-46be-826b-770a90b8899e{ip: 192.168.151.82, host: pr-hdds-2162-8g7kw-3348123894, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-28 05:55:45,059 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: addNew group-DECF01526FF4:[86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635] returns group-DECF01526FF4:java.util.concurrent.CompletableFuture@4faf814e[Not completed]
2019-09-28 05:55:45,061 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: new RaftServerImpl for group-DECF01526FF4:[86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635] with ContainerStateMachine:uninitialized
2019-09-28 05:55:45,061 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-28 05:55:45,061 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-28 05:55:45,062 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-28 05:55:45,062 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-28 05:55:45,062 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-28 05:55:45,062 [pool-37-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4: ConfigurationManager, init=-1: [86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null, confs=<EMPTY_MAP>
2019-09-28 05:55:45,062 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/ratis] (custom)
2019-09-28 05:55:45,063 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/ratis/5be8d622-4e33-4597-9030-decf01526ff4 does not exist. Creating ...
2019-09-28 05:55:45,076 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/ratis/5be8d622-4e33-4597-9030-decf01526ff4/in_use.lock acquired by nodename 21794@pr-hdds-2162-8g7kw-3348123894
2019-09-28 05:55:45,089 [pool-37-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/ratis/5be8d622-4e33-4597-9030-decf01526ff4 has been successfully formatted.
2019-09-28 05:55:45,089 [pool-37-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-DECF01526FF4: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-28 05:55:45,089 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-28 05:55:45,089 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-28 05:55:45,090 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-28 05:55:45,090 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:45,090 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:45,090 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-28 05:55:45,090 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/ratis/5be8d622-4e33-4597-9030-decf01526ff4
2019-09-28 05:55:45,094 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-28 05:55:45,094 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-28 05:55:45,095 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:45,095 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-28 05:55:45,095 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-28 05:55:45,095 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-28 05:55:45,095 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-28 05:55:45,095 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-28 05:55:45,096 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-28 05:55:45,096 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-28 05:55:45,096 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-28 05:55:45,097 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-28 05:55:45,097 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-28 05:55:45,097 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-28 05:55:45,097 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-28 05:55:45,101 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4: start as a follower, conf=-1: [86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 05:55:45,101 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-28 05:55:45,101 [pool-37-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: start FollowerState
2019-09-28 05:55:45,101 [pool-37-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DECF01526FF4,id=86ebeb77-e9e6-4e56-959d-17a96123c217
2019-09-28 05:55:45,113 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 5be8d622-4e33-4597-9030-decf01526ff4, Nodes: 86ebeb77-e9e6-4e56-959d-17a96123c217{ip: 192.168.151.82, host: pr-hdds-2162-8g7kw-3348123894, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-28 05:55:45,137 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 301de80a-213b-4abd-bdb2-9b127524bed9: addNew group-51BD14811DA0:[301de80a-213b-4abd-bdb2-9b127524bed9:192.168.151.82:38999] returns group-51BD14811DA0:java.util.concurrent.CompletableFuture@4cf98487[Not completed]
2019-09-28 05:55:45,140 [pool-67-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 301de80a-213b-4abd-bdb2-9b127524bed9: new RaftServerImpl for group-51BD14811DA0:[301de80a-213b-4abd-bdb2-9b127524bed9:192.168.151.82:38999] with ContainerStateMachine:uninitialized
2019-09-28 05:55:45,140 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-28 05:55:45,140 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-28 05:55:45,141 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-28 05:55:45,141 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-28 05:55:45,141 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-28 05:55:45,141 [pool-67-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0: ConfigurationManager, init=-1: [301de80a-213b-4abd-bdb2-9b127524bed9:192.168.151.82:38999], old=null, confs=<EMPTY_MAP>
2019-09-28 05:55:45,141 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-4/data/ratis] (custom)
2019-09-28 05:55:45,141 [pool-67-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-4/data/ratis/648fd0c2-2c0b-4d33-91d3-51bd14811da0 does not exist. Creating ...
2019-09-28 05:55:45,155 [pool-67-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-4/data/ratis/648fd0c2-2c0b-4d33-91d3-51bd14811da0/in_use.lock acquired by nodename 21794@pr-hdds-2162-8g7kw-3348123894
2019-09-28 05:55:45,168 [pool-67-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-4/data/ratis/648fd0c2-2c0b-4d33-91d3-51bd14811da0 has been successfully formatted.
2019-09-28 05:55:45,168 [pool-67-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-51BD14811DA0: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-28 05:55:45,168 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-28 05:55:45,168 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-28 05:55:45,168 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-28 05:55:45,169 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:45,169 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:45,169 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-28 05:55:45,169 [pool-67-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-4/data/ratis/648fd0c2-2c0b-4d33-91d3-51bd14811da0
2019-09-28 05:55:45,172 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-28 05:55:45,172 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-28 05:55:45,173 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:45,173 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-28 05:55:45,173 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-28 05:55:45,173 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-28 05:55:45,173 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-28 05:55:45,174 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-28 05:55:45,174 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-28 05:55:45,174 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-28 05:55:45,174 [pool-67-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-28 05:55:45,175 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-28 05:55:45,175 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-28 05:55:45,175 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-28 05:55:45,175 [pool-67-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-28 05:55:45,178 [pool-67-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0: start as a follower, conf=-1: [301de80a-213b-4abd-bdb2-9b127524bed9:192.168.151.82:38999], old=null
2019-09-28 05:55:45,178 [pool-67-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-28 05:55:45,179 [pool-67-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 301de80a-213b-4abd-bdb2-9b127524bed9: start FollowerState
2019-09-28 05:55:45,179 [pool-67-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-51BD14811DA0,id=301de80a-213b-4abd-bdb2-9b127524bed9
2019-09-28 05:55:45,190 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 648fd0c2-2c0b-4d33-91d3-51bd14811da0, Nodes: 301de80a-213b-4abd-bdb2-9b127524bed9{ip: 192.168.151.82, host: pr-hdds-2162-8g7kw-3348123894, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]
2019-09-28 05:55:45,231 [grpc-default-executor-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - bc198d42-aabb-46be-826b-770a90b8899e: addNew group-157546BB1BA7:[bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635] returns group-157546BB1BA7:java.util.concurrent.CompletableFuture@6a26f2a3[Not completed]
2019-09-28 05:55:45,232 [grpc-default-executor-2] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: addNew group-157546BB1BA7:[bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635] returns group-157546BB1BA7:java.util.concurrent.CompletableFuture@75743e8[Not completed]
2019-09-28 05:55:45,232 [grpc-default-executor-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: addNew group-157546BB1BA7:[bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635] returns group-157546BB1BA7:java.util.concurrent.CompletableFuture@455b7cc2[Not completed]
2019-09-28 05:55:45,234 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: new RaftServerImpl for group-157546BB1BA7:[bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635] with ContainerStateMachine:uninitialized
2019-09-28 05:55:45,234 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-28 05:55:45,234 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-28 05:55:45,234 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-28 05:55:45,234 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-28 05:55:45,234 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: new RaftServerImpl for group-157546BB1BA7:[bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635] with ContainerStateMachine:uninitialized
2019-09-28 05:55:45,235 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-28 05:55:45,235 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-28 05:55:45,235 [pool-57-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7: ConfigurationManager, init=-1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null, confs=<EMPTY_MAP>
2019-09-28 05:55:45,235 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-28 05:55:45,235 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-28 05:55:45,235 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/ratis] (custom)
2019-09-28 05:55:45,235 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-28 05:55:45,235 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-28 05:55:45,235 [pool-37-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7: ConfigurationManager, init=-1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null, confs=<EMPTY_MAP>
2019-09-28 05:55:45,235 [pool-57-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7 does not exist. Creating ...
2019-09-28 05:55:45,236 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/ratis] (custom)
2019-09-28 05:55:45,236 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7 does not exist. Creating ...
2019-09-28 05:55:45,236 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(95)) - bc198d42-aabb-46be-826b-770a90b8899e: new RaftServerImpl for group-157546BB1BA7:[bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635] with ContainerStateMachine:uninitialized
2019-09-28 05:55:45,236 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.min = 5s (custom)
2019-09-28 05:55:45,237 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.timeout.max = 5200ms (custom)
2019-09-28 05:55:45,237 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpcslowness.timeout = 120s (custom)
2019-09-28 05:55:45,237 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.sleep.deviation.threshold = 300 (default)
2019-09-28 05:55:45,237 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-28 05:55:45,237 [pool-47-thread-1] INFO  impl.RaftServerImpl (ServerState.java:<init>(103)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7: ConfigurationManager, init=-1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null, confs=<EMPTY_MAP>
2019-09-28 05:55:45,238 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.storage.dir = [/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/ratis] (custom)
2019-09-28 05:55:45,238 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:analyzeStorage(246)) - The storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7 does not exist. Creating ...
2019-09-28 05:55:45,249 [pool-57-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7/in_use.lock acquired by nodename 21794@pr-hdds-2162-8g7kw-3348123894
2019-09-28 05:55:45,249 [pool-47-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7/in_use.lock acquired by nodename 21794@pr-hdds-2162-8g7kw-3348123894
2019-09-28 05:55:45,249 [pool-37-thread-1] INFO  storage.RaftStorageDirectory (RaftStorageDirectory.java:tryLock(328)) - Lock on /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7/in_use.lock acquired by nodename 21794@pr-hdds-2162-8g7kw-3348123894
2019-09-28 05:55:45,264 [pool-57-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7 has been successfully formatted.
2019-09-28 05:55:45,264 [pool-37-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7 has been successfully formatted.
2019-09-28 05:55:45,264 [pool-47-thread-1] INFO  storage.RaftStorage (RaftStorage.java:format(72)) - Storage directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7 has been successfully formatted.
2019-09-28 05:55:45,264 [pool-57-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-157546BB1BA7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-28 05:55:45,264 [pool-37-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-157546BB1BA7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-28 05:55:45,264 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-28 05:55:45,264 [pool-47-thread-1] INFO  ratis.ContainerStateMachine (ContainerStateMachine.java:loadSnapshot(230)) - group-157546BB1BA7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
2019-09-28 05:55:45,265 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-28 05:55:45,265 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-28 05:55:45,265 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-28 05:55:45,265 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.notification.no-leader.timeout = 120s (custom)
2019-09-28 05:55:45,265 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:45,265 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-28 05:55:45,265 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:45,265 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.use.memory = false (default)
2019-09-28 05:55:45,266 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-28 05:55:45,266 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-28 05:55:45,266 [pool-57-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7
2019-09-28 05:55:45,266 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.purge.gap = 1000000 (custom)
2019-09-28 05:55:45,266 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-28 05:55:45,266 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:45,266 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-28 05:55:45,266 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:45,267 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:45,267 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:45,267 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-28 05:55:45,267 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:45,267 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-28 05:55:45,267 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-28 05:55:45,267 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-28 05:55:45,268 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-28 05:55:45,267 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.cache.num.max = 2 (custom)
2019-09-28 05:55:45,268 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-28 05:55:45,268 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7
2019-09-28 05:55:45,268 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-28 05:55:45,268 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:<init>(172)) - new bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7-SegmentedRaftLogWorker for RaftStorage:Storage Directory /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7
2019-09-28 05:55:45,268 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-28 05:55:45,268 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-28 05:55:45,269 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.byte-limit = 2147483647 (custom)
2019-09-28 05:55:45,269 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-28 05:55:45,269 [pool-57-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-28 05:55:45,269 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:45,269 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.queue.element-limit = 1024 (custom)
2019-09-28 05:55:45,270 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-28 05:55:45,269 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-28 05:55:45,270 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-28 05:55:45,270 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.segment.size.max = 1048576 (custom)
2019-09-28 05:55:45,270 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-28 05:55:45,270 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-28 05:55:45,270 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-28 05:55:45,270 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.preallocated.size = 16384 (custom)
2019-09-28 05:55:45,271 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-28 05:55:45,271 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-28 05:55:45,271 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-28 05:55:45,271 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.write.buffer.size = 33554432 (custom)
2019-09-28 05:55:45,271 [pool-57-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-28 05:55:45,272 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-28 05:55:45,272 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.force.sync.num = 128 (default)
2019-09-28 05:55:45,272 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync = true (default)
2019-09-28 05:55:45,272 [pool-37-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-28 05:55:45,272 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout = 10s (default)
2019-09-28 05:55:45,272 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2019-09-28 05:55:45,273 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-28 05:55:45,273 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.statemachine.data.caching.enabled = true (custom)
2019-09-28 05:55:45,273 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-28 05:55:45,273 [pool-47-thread-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:lambda$new$0(132)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2019-09-28 05:55:45,273 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-28 05:55:45,273 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.enabled = true (custom)
2019-09-28 05:55:45,274 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
2019-09-28 05:55:45,274 [pool-37-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-28 05:55:45,274 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.snapshot.retention.num.files = 5 (custom)
2019-09-28 05:55:45,274 [pool-47-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.retrycache.expirytime = 600000ms (custom)
2019-09-28 05:55:45,275 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7: start as a follower, conf=-1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 05:55:45,275 [pool-57-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-28 05:55:45,275 [pool-57-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: start FollowerState
2019-09-28 05:55:45,275 [pool-57-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-157546BB1BA7,id=6f360f6f-30d2-485d-bdb1-20146522ae70
2019-09-28 05:55:45,280 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7: start as a follower, conf=-1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 05:55:45,280 [pool-47-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-28 05:55:45,281 [pool-47-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - bc198d42-aabb-46be-826b-770a90b8899e: start FollowerState
2019-09-28 05:55:45,281 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:start(185)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7: start as a follower, conf=-1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 05:55:45,281 [pool-47-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-157546BB1BA7,id=bc198d42-aabb-46be-826b-770a90b8899e
2019-09-28 05:55:45,282 [pool-37-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7: changes role from      null to FOLLOWER at term 0 for startAsFollower
2019-09-28 05:55:45,283 [pool-37-thread-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: start FollowerState
2019-09-28 05:55:45,284 [pool-37-thread-1] INFO  util.JmxRegister (JmxRegister.java:tryRegister(44)) - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-157546BB1BA7,id=86ebeb77-e9e6-4e56-959d-17a96123c217
2019-09-28 05:55:45,301 [RatisPipelineUtilsThread] INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(56)) - Created pipeline Pipeline[ Id: 215e8ef8-9577-4e18-8695-157546bb1ba7, Nodes: 6f360f6f-30d2-485d-bdb1-20146522ae70{ip: 192.168.151.82, host: pr-hdds-2162-8g7kw-3348123894, networkLocation: /default-rack, certSerialId: null}bc198d42-aabb-46be-826b-770a90b8899e{ip: 192.168.151.82, host: pr-hdds-2162-8g7kw-3348123894, networkLocation: /default-rack, certSerialId: null}86ebeb77-e9e6-4e56-959d-17a96123c217{ip: 192.168.151.82, host: pr-hdds-2162-8g7kw-3348123894, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]
2019-09-28 05:55:45,356 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(147)) - Cluster is ready. Got 5 of 5 DN Heartbeats.
2019-09-28 05:55:46,594 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:55:47,023 [Thread-208] INFO  container.ReplicationManager (ReplicationManager.java:start(162)) - Starting Replication Monitor Thread.
2019-09-28 05:55:47,027 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 1 milliseconds for processing 0 containers.
2019-09-28 05:55:47,595 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:55:48,597 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:55:49,598 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:55:49,944 [Thread-210] INFO  impl.FollowerState (FollowerState.java:run(106)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8:group-E59B61FA56EE changes to CANDIDATE, lastRpcTime:5195, electionTimeout:5194ms
2019-09-28 05:55:49,947 [Thread-210] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8: shutdown FollowerState
2019-09-28 05:55:49,948 [Thread-210] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-28 05:55:49,957 [Thread-210] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8: start LeaderElection
2019-09-28 05:55:49,976 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1: begin an election at term 1 for -1: [1db3c405-2e55-4b17-b9d1-8ec21761eee8:192.168.151.82:33114], old=null
2019-09-28 05:55:49,978 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8: shutdown LeaderElection
2019-09-28 05:55:49,979 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-28 05:55:49,979 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE: change Leader from null to 1db3c405-2e55-4b17-b9d1-8ec21761eee8 at term 1 for becomeLeader, leader elected after 5373ms
2019-09-28 05:55:49,988 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-28 05:55:49,989 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-28 05:55:49,992 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-28 05:55:49,995 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-28 05:55:49,996 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-28 05:55:49,997 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-28 05:55:50,014 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8: start LeaderState
2019-09-28 05:55:50,042 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-28 05:55:50,052 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE:LeaderElection1] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE: set configuration 0: [1db3c405-2e55-4b17-b9d1-8ec21761eee8:192.168.151.82:33114], old=null at 0
2019-09-28 05:55:50,101 [Thread-213] INFO  impl.FollowerState (FollowerState.java:run(106)) - 6f360f6f-30d2-485d-bdb1-20146522ae70:group-36418E16FEF6 changes to CANDIDATE, lastRpcTime:5189, electionTimeout:5188ms
2019-09-28 05:55:50,103 [Thread-213] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: shutdown FollowerState
2019-09-28 05:55:50,103 [Thread-213] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-28 05:55:50,104 [Thread-213] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: start LeaderElection
2019-09-28 05:55:50,108 [Thread-216] INFO  impl.FollowerState (FollowerState.java:run(106)) - bc198d42-aabb-46be-826b-770a90b8899e:group-707AEF46CAE3 changes to CANDIDATE, lastRpcTime:5078, electionTimeout:5077ms
2019-09-28 05:55:50,108 [Thread-216] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - bc198d42-aabb-46be-826b-770a90b8899e: shutdown FollowerState
2019-09-28 05:55:50,108 [Thread-216] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-28 05:55:50,108 [Thread-216] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - bc198d42-aabb-46be-826b-770a90b8899e: start LeaderElection
2019-09-28 05:55:50,113 [Thread-219] INFO  impl.FollowerState (FollowerState.java:run(106)) - 86ebeb77-e9e6-4e56-959d-17a96123c217:group-DECF01526FF4 changes to CANDIDATE, lastRpcTime:5011, electionTimeout:5011ms
2019-09-28 05:55:50,113 [Thread-219] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: shutdown FollowerState
2019-09-28 05:55:50,113 [Thread-219] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-28 05:55:50,113 [Thread-219] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: start LeaderElection
2019-09-28 05:55:50,122 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3: begin an election at term 1 for -1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666], old=null
2019-09-28 05:55:50,122 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4: begin an election at term 1 for -1: [86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 05:55:50,133 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2: begin an election at term 1 for -1: [6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933], old=null
2019-09-28 05:55:50,133 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: shutdown LeaderElection
2019-09-28 05:55:50,133 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - bc198d42-aabb-46be-826b-770a90b8899e: shutdown LeaderElection
2019-09-28 05:55:50,134 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-28 05:55:50,134 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4: change Leader from null to 86ebeb77-e9e6-4e56-959d-17a96123c217 at term 1 for becomeLeader, leader elected after 5044ms
2019-09-28 05:55:50,134 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: shutdown LeaderElection
2019-09-28 05:55:50,135 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-28 05:55:50,134 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-28 05:55:50,136 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-28 05:55:50,136 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-28 05:55:50,136 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-28 05:55:50,136 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3: change Leader from null to bc198d42-aabb-46be-826b-770a90b8899e at term 1 for becomeLeader, leader elected after 5139ms
2019-09-28 05:55:50,137 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-28 05:55:50,136 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6: change Leader from null to 6f360f6f-30d2-485d-bdb1-20146522ae70 at term 1 for becomeLeader, leader elected after 5237ms
2019-09-28 05:55:50,138 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-28 05:55:50,138 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-28 05:55:50,140 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-28 05:55:50,140 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-28 05:55:50,140 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-28 05:55:50,140 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-28 05:55:50,141 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-28 05:55:50,141 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-28 05:55:50,141 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-28 05:55:50,141 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-28 05:55:50,141 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-28 05:55:50,141 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-28 05:55:50,141 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-28 05:55:50,142 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-28 05:55:50,143 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: start LeaderState
2019-09-28 05:55:50,143 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-28 05:55:50,144 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4:LeaderElection4] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4: set configuration 0: [86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null at 0
2019-09-28 05:55:50,149 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - bc198d42-aabb-46be-826b-770a90b8899e: start LeaderState
2019-09-28 05:55:50,149 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: start LeaderState
2019-09-28 05:55:50,149 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-28 05:55:50,149 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-28 05:55:50,150 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3:LeaderElection3] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3: set configuration 0: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666], old=null at 0
2019-09-28 05:55:50,151 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6:LeaderElection2] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6: set configuration 0: [6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933], old=null at 0
2019-09-28 05:55:50,248 [Thread-222] INFO  impl.FollowerState (FollowerState.java:run(106)) - 301de80a-213b-4abd-bdb2-9b127524bed9:group-51BD14811DA0 changes to CANDIDATE, lastRpcTime:5069, electionTimeout:5069ms
2019-09-28 05:55:50,249 [Thread-222] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 301de80a-213b-4abd-bdb2-9b127524bed9: shutdown FollowerState
2019-09-28 05:55:50,249 [Thread-222] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-28 05:55:50,249 [Thread-222] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 301de80a-213b-4abd-bdb2-9b127524bed9: start LeaderElection
2019-09-28 05:55:50,269 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5: begin an election at term 1 for -1: [301de80a-213b-4abd-bdb2-9b127524bed9:192.168.151.82:38999], old=null
2019-09-28 05:55:50,270 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 301de80a-213b-4abd-bdb2-9b127524bed9: shutdown LeaderElection
2019-09-28 05:55:50,270 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2019-09-28 05:55:50,270 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0: change Leader from null to 301de80a-213b-4abd-bdb2-9b127524bed9 at term 1 for becomeLeader, leader elected after 5101ms
2019-09-28 05:55:50,305 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-28 05:55:50,305 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-28 05:55:50,306 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-28 05:55:50,306 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-28 05:55:50,306 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-28 05:55:50,306 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-28 05:55:50,310 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 301de80a-213b-4abd-bdb2-9b127524bed9: start LeaderState
2019-09-28 05:55:50,310 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-28 05:55:50,311 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0:LeaderElection5] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0: set configuration 0: [301de80a-213b-4abd-bdb2-9b127524bed9:192.168.151.82:38999], old=null at 0
2019-09-28 05:55:50,340 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/ratis/143d12b9-c00c-4eb4-ae18-707aef46cae3/current/log_inprogress_0
2019-09-28 05:55:50,340 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/ratis/5be8d622-4e33-4597-9030-decf01526ff4/current/log_inprogress_0
2019-09-28 05:55:50,340 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/ratis/098a7288-c4bf-417c-8332-36418e16fef6/current/log_inprogress_0
2019-09-28 05:55:50,340 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-0/data/ratis/05001a20-6613-4f60-ad80-e59b61fa56ee/current/log_inprogress_0
2019-09-28 05:55:50,343 [Thread-229] INFO  impl.FollowerState (FollowerState.java:run(106)) - bc198d42-aabb-46be-826b-770a90b8899e:group-157546BB1BA7 changes to CANDIDATE, lastRpcTime:5062, electionTimeout:5062ms
2019-09-28 05:55:50,343 [Thread-229] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - bc198d42-aabb-46be-826b-770a90b8899e: shutdown FollowerState
2019-09-28 05:55:50,343 [Thread-229] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-28 05:55:50,344 [Thread-229] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - bc198d42-aabb-46be-826b-770a90b8899e: start LeaderElection
2019-09-28 05:55:50,353 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-4/data/ratis/648fd0c2-2c0b-4d33-91d3-51bd14811da0/current/log_inprogress_0
2019-09-28 05:55:50,353 [bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7:LeaderElection6] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7:LeaderElection6: begin an election at term 1 for -1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 05:55:50,354 [Thread-227] INFO  impl.FollowerState (FollowerState.java:run(106)) - 6f360f6f-30d2-485d-bdb1-20146522ae70:group-157546BB1BA7 changes to CANDIDATE, lastRpcTime:5078, electionTimeout:5078ms
2019-09-28 05:55:50,354 [Thread-227] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: shutdown FollowerState
2019-09-28 05:55:50,354 [Thread-227] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-28 05:55:50,356 [Thread-227] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: start LeaderElection
2019-09-28 05:55:50,363 [Thread-230] INFO  impl.FollowerState (FollowerState.java:run(106)) - 86ebeb77-e9e6-4e56-959d-17a96123c217:group-157546BB1BA7 changes to CANDIDATE, lastRpcTime:5079, electionTimeout:5079ms
2019-09-28 05:55:50,363 [Thread-230] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: shutdown FollowerState
2019-09-28 05:55:50,363 [Thread-230] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2019-09-28 05:55:50,363 [Thread-230] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: start LeaderElection
2019-09-28 05:55:50,367 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection7] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection7: begin an election at term 1 for -1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 05:55:50,371 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection8] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection8: begin an election at term 1 for -1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 05:55:50,427 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection7] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(56)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection7: Election REJECTED; received 2 response(s) [6f360f6f-30d2-485d-bdb1-20146522ae70<-bc198d42-aabb-46be-826b-770a90b8899e#0:FAIL-t1, 6f360f6f-30d2-485d-bdb1-20146522ae70<-86ebeb77-e9e6-4e56-959d-17a96123c217#0:FAIL-t1] and 0 exception(s); 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:t1, leader=null, voted=6f360f6f-30d2-485d-bdb1-20146522ae70, raftlog=6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 05:55:50,432 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection7] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7: changes role from CANDIDATE to FOLLOWER at term 1 for DISCOVERED_A_NEW_TERM
2019-09-28 05:55:50,432 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection7] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: shutdown LeaderElection
2019-09-28 05:55:50,433 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection7] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: start FollowerState
2019-09-28 05:55:50,434 [bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7:LeaderElection6] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(56)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7:LeaderElection6: Election REJECTED; received 2 response(s) [bc198d42-aabb-46be-826b-770a90b8899e<-6f360f6f-30d2-485d-bdb1-20146522ae70#0:FAIL-t1, bc198d42-aabb-46be-826b-770a90b8899e<-86ebeb77-e9e6-4e56-959d-17a96123c217#0:FAIL-t1] and 0 exception(s); bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7:t1, leader=null, voted=bc198d42-aabb-46be-826b-770a90b8899e, raftlog=bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 05:55:50,436 [bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7:LeaderElection6] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7: changes role from CANDIDATE to FOLLOWER at term 1 for DISCOVERED_A_NEW_TERM
2019-09-28 05:55:50,438 [bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7:LeaderElection6] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - bc198d42-aabb-46be-826b-770a90b8899e: shutdown LeaderElection
2019-09-28 05:55:50,439 [bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7:LeaderElection6] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - bc198d42-aabb-46be-826b-770a90b8899e: start FollowerState
2019-09-28 05:55:50,442 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection8] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(56)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection8: Election REJECTED; received 2 response(s) [86ebeb77-e9e6-4e56-959d-17a96123c217<-bc198d42-aabb-46be-826b-770a90b8899e#0:FAIL-t1, 86ebeb77-e9e6-4e56-959d-17a96123c217<-6f360f6f-30d2-485d-bdb1-20146522ae70#0:FAIL-t1] and 0 exception(s); 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:t1, leader=null, voted=86ebeb77-e9e6-4e56-959d-17a96123c217, raftlog=86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 05:55:50,443 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection8] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7: changes role from CANDIDATE to FOLLOWER at term 1 for DISCOVERED_A_NEW_TERM
2019-09-28 05:55:50,446 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection8] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: shutdown LeaderElection
2019-09-28 05:55:50,446 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection8] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: start FollowerState
2019-09-28 05:55:50,599 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:55:51,602 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:55:52,604 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:55:53,605 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:55:54,606 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:55:55,483 [Thread-256] INFO  impl.FollowerState (FollowerState.java:run(106)) - 86ebeb77-e9e6-4e56-959d-17a96123c217:group-157546BB1BA7 changes to CANDIDATE, lastRpcTime:5037, electionTimeout:5035ms
2019-09-28 05:55:55,483 [Thread-256] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: shutdown FollowerState
2019-09-28 05:55:55,484 [Thread-256] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
2019-09-28 05:55:55,484 [Thread-256] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: start LeaderElection
2019-09-28 05:55:55,501 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9: begin an election at term 2 for -1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 05:55:55,508 [grpc-default-executor-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7: changes role from  FOLLOWER to FOLLOWER at term 2 for recognizeCandidate:86ebeb77-e9e6-4e56-959d-17a96123c217
2019-09-28 05:55:55,508 [grpc-default-executor-2] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7: changes role from  FOLLOWER to FOLLOWER at term 2 for recognizeCandidate:86ebeb77-e9e6-4e56-959d-17a96123c217
2019-09-28 05:55:55,508 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - bc198d42-aabb-46be-826b-770a90b8899e: shutdown FollowerState
2019-09-28 05:55:55,508 [grpc-default-executor-2] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: shutdown FollowerState
2019-09-28 05:55:55,508 [Thread-255] INFO  impl.FollowerState (FollowerState.java:run(115)) - bc198d42-aabb-46be-826b-770a90b8899e: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-28 05:55:55,508 [grpc-default-executor-1] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - bc198d42-aabb-46be-826b-770a90b8899e: start FollowerState
2019-09-28 05:55:55,509 [grpc-default-executor-2] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: start FollowerState
2019-09-28 05:55:55,509 [Thread-254] INFO  impl.FollowerState (FollowerState.java:run(115)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-28 05:55:55,535 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(56)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9: Election PASSED; received 1 response(s) [86ebeb77-e9e6-4e56-959d-17a96123c217<-6f360f6f-30d2-485d-bdb1-20146522ae70#0:OK-t2] and 0 exception(s); 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:t2, leader=null, voted=86ebeb77-e9e6-4e56-959d-17a96123c217, raftlog=86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 05:55:55,535 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: shutdown LeaderElection
2019-09-28 05:55:55,537 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
2019-09-28 05:55:55,537 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7: change Leader from null to 86ebeb77-e9e6-4e56-959d-17a96123c217 at term 2 for becomeLeader, leader elected after 10272ms
2019-09-28 05:55:55,537 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.staging.catchup.gap = 1000 (default)
2019-09-28 05:55:55,537 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.sleep.time = 25ms (default)
2019-09-28 05:55:55,538 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.write.element-limit = 4096 (default)
2019-09-28 05:55:55,538 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout = 10s (default)
2019-09-28 05:55:55,538 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.timeout.denomination = 1s (default)
2019-09-28 05:55:55,538 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.watch.element-limit = 65536 (default)
2019-09-28 05:55:55,544 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2019-09-28 05:55:55,544 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:55,545 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2019-09-28 05:55:55,549 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2019-09-28 05:55:55,549 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-28 05:55:55,549 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-28 05:55:55,550 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
2019-09-28 05:55:55,551 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2019-09-28 05:55:55,551 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.buffer.element-limit = 1 (custom)
2019-09-28 05:55:55,552 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  grpc.GrpcConfigKeys$Server (ConfUtils.java:logGet(43)) - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
2019-09-28 05:55:55,552 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.rpc.request.timeout = 3000ms (default)
2019-09-28 05:55:55,552 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(43)) - raft.server.log.appender.install.snapshot.enabled = false (custom)
2019-09-28 05:55:55,555 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: start LeaderState
2019-09-28 05:55:55,555 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-28 05:55:55,556 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7:LeaderElection9] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7: set configuration 0: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null at 0
2019-09-28 05:55:55,600 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7/current/log_inprogress_0
2019-09-28 05:55:55,608 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:55:55,616 [grpc-default-executor-2] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7: change Leader from null to 86ebeb77-e9e6-4e56-959d-17a96123c217 at term 2 for appendEntries, leader elected after 10351ms
2019-09-28 05:55:55,616 [grpc-default-executor-1] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7: change Leader from null to 86ebeb77-e9e6-4e56-959d-17a96123c217 at term 2 for appendEntries, leader elected after 10351ms
2019-09-28 05:55:55,643 [grpc-default-executor-2] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7: set configuration 0: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null at 0
2019-09-28 05:55:55,643 [grpc-default-executor-1] INFO  impl.RaftServerImpl (ServerState.java:setRaftConf(354)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7: set configuration 0: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null at 0
2019-09-28 05:55:55,643 [grpc-default-executor-2] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-28 05:55:55,644 [grpc-default-executor-1] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:startLogSegment(380)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7-SegmentedRaftLogWorker: Starting segment from index:0
2019-09-28 05:55:55,676 [bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7/current/log_inprogress_0
2019-09-28 05:55:55,676 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:execute(569)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7-SegmentedRaftLogWorker: created new log segment /workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/ratis/215e8ef8-9577-4e18-8695-157546bb1ba7/current/log_inprogress_0
2019-09-28 05:55:56,616 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:55:57,618 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:55:58,619 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:55:59,620 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:00,621 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:01,623 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:02,624 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:03,626 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:04,627 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:05,629 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:05,631 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 1 failover attempts. Trying to failover immediately.
2019-09-28 05:56:06,632 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:07,633 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:08,635 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:09,636 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:10,637 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:11,639 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:12,640 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:13,641 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:14,643 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:15,644 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:15,645 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 2 failover attempts. Trying to failover immediately.
2019-09-28 05:56:16,647 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:17,648 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:18,649 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:19,650 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:20,652 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:21,653 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:22,654 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:23,655 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:24,657 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:25,658 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:25,659 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 3 failover attempts. Trying to failover immediately.
2019-09-28 05:56:26,660 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:27,661 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:28,663 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:29,664 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:30,665 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:31,667 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:32,668 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:33,669 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:34,671 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:35,672 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:35,673 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 4 failover attempts. Trying to failover immediately.
2019-09-28 05:56:36,674 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:37,676 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:38,677 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:39,678 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:40,679 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:41,681 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:42,682 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:43,684 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:44,686 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:45,687 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:45,688 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 5 failover attempts. Trying to failover immediately.
2019-09-28 05:56:46,690 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:47,691 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:48,692 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:49,693 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:50,694 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:51,695 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:52,696 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:53,697 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:54,698 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:55,699 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:55,700 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 6 failover attempts. Trying to failover immediately.
2019-09-28 05:56:56,701 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:57,702 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:58,703 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:56:59,705 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:00,706 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:01,707 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:02,708 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:03,709 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:04,710 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:05,711 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:05,712 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 7 failover attempts. Trying to failover immediately.
2019-09-28 05:57:06,713 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:07,714 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:08,715 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:09,716 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:10,717 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:11,718 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:12,719 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:13,720 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:14,721 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:15,722 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:15,724 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 8 failover attempts. Trying to failover immediately.
2019-09-28 05:57:16,725 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:17,726 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:18,727 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:19,728 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:20,729 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:21,730 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:22,731 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:23,732 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:24,733 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:25,734 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:25,735 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 9 failover attempts. Trying to failover immediately.
2019-09-28 05:57:26,736 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:27,737 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:28,738 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:29,739 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:30,740 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:31,741 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:32,742 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:33,743 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:34,744 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:35,745 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:35,746 [main] ERROR ha.OMFailoverProxyProvider (OzoneManagerProtocolClientSideTranslatorPB.java:getRetryAction(268)) - Failed to connect to OM. Attempted 10 retries and 10 failovers
2019-09-28 05:57:35,748 [main] ERROR client.OzoneClientFactory (OzoneClientFactory.java:getClientProtocol(259)) - Couldn't create RpcClient protocol exception: 
java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy36.submitRequest(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy36.submitRequest(Unknown Source)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:338)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1223)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
	at com.sun.proxy.$Proxy37.getServiceInfo(Unknown Source)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:154)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:256)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:239)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClient(OzoneClientFactory.java:75)
	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.init(RandomKeyGenerator.java:242)
	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.call(RandomKeyGenerator.java:262)
	at org.apache.hadoop.ozone.freon.TestDataValidate.ratisTestLargeKey(TestDataValidate.java:69)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 56 more
2019-09-28 05:57:36,762 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:37,763 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:38,764 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:39,765 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:40,766 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:41,767 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:42,768 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:43,770 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:44,771 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:45,772 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:46,774 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:47,775 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:48,776 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:49,777 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:50,779 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:51,780 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:52,780 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:53,782 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:54,783 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:55,784 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:55,785 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 1 failover attempts. Trying to failover immediately.
2019-09-28 05:57:56,786 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:57,787 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:58,787 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:57:59,789 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:00,790 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:01,790 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:02,791 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:03,793 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:04,794 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:05,795 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:05,796 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 2 failover attempts. Trying to failover immediately.
2019-09-28 05:58:06,797 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:07,798 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:08,800 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:09,802 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:10,802 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:11,803 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:12,804 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:13,805 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:14,806 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:15,807 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:15,808 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 3 failover attempts. Trying to failover immediately.
2019-09-28 05:58:16,809 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:17,810 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:18,811 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:19,812 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:20,813 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:21,814 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:22,815 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:23,817 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:24,817 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:25,819 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:25,820 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 4 failover attempts. Trying to failover immediately.
2019-09-28 05:58:26,821 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:27,822 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:28,824 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:29,825 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:30,826 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:31,827 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:32,828 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:33,830 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:34,831 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:35,832 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:35,834 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 5 failover attempts. Trying to failover immediately.
2019-09-28 05:58:36,835 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:37,836 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:38,838 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:39,839 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:40,840 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:41,841 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:42,842 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:43,843 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:44,844 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:45,845 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:45,847 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 6 failover attempts. Trying to failover immediately.
2019-09-28 05:58:46,859 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:47,861 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:48,863 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:49,865 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:50,865 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:51,867 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:52,868 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:53,869 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:54,870 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:55,871 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:55,872 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 7 failover attempts. Trying to failover immediately.
2019-09-28 05:58:56,873 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:57,874 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:58,876 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:58:59,877 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:00,878 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:01,882 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:02,883 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:03,885 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:04,886 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:05,888 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:05,889 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 8 failover attempts. Trying to failover immediately.
2019-09-28 05:59:06,892 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:07,895 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:08,896 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:09,897 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:10,899 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:11,901 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:12,902 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:13,903 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:14,904 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:15,905 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:15,906 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 9 failover attempts. Trying to failover immediately.
2019-09-28 05:59:16,907 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:17,908 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:18,909 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:19,911 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:20,911 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:21,913 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:22,914 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:23,916 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:24,917 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:25,918 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:25,919 [main] ERROR ha.OMFailoverProxyProvider (OzoneManagerProtocolClientSideTranslatorPB.java:getRetryAction(268)) - Failed to connect to OM. Attempted 10 retries and 10 failovers
2019-09-28 05:59:25,919 [main] ERROR client.OzoneClientFactory (OzoneClientFactory.java:getClientProtocol(259)) - Couldn't create RpcClient protocol exception: 
java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy36.submitRequest(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy36.submitRequest(Unknown Source)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:338)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1223)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
	at com.sun.proxy.$Proxy37.getServiceInfo(Unknown Source)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:154)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:256)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:239)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClient(OzoneClientFactory.java:75)
	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.init(RandomKeyGenerator.java:242)
	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.call(RandomKeyGenerator.java:262)
	at org.apache.hadoop.ozone.freon.TestDataValidate.standaloneTestLargeKey(TestDataValidate.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 55 more
2019-09-28 05:59:26,924 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:27,925 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:28,926 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:29,927 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:30,928 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:31,930 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:32,931 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:33,932 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:34,933 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:35,934 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:36,936 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:37,938 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:38,939 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:39,941 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:40,942 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:41,944 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:42,945 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:43,946 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:44,947 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:45,948 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:45,949 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 1 failover attempts. Trying to failover immediately.
2019-09-28 05:59:46,950 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:47,951 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:48,952 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:49,953 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:50,954 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:51,955 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:52,956 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:53,957 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:54,958 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:55,958 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:55,960 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 2 failover attempts. Trying to failover immediately.
2019-09-28 05:59:56,961 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:57,962 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:58,963 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 05:59:59,964 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:00,965 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:01,967 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:02,968 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:03,969 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:04,970 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:05,971 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:05,972 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 3 failover attempts. Trying to failover immediately.
2019-09-28 06:00:06,973 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:07,975 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:08,976 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:09,977 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:10,978 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:11,979 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:12,980 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:13,981 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:14,982 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:15,983 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:15,984 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 4 failover attempts. Trying to failover immediately.
2019-09-28 06:00:16,985 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:17,986 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:18,986 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:19,987 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:20,988 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:21,989 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:22,990 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:23,990 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:24,991 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:25,992 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:25,993 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 5 failover attempts. Trying to failover immediately.
2019-09-28 06:00:26,994 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:27,995 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:28,996 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:29,997 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:30,998 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:31,999 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:33,000 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:34,001 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:35,002 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:36,002 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:36,004 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 6 failover attempts. Trying to failover immediately.
2019-09-28 06:00:37,005 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:38,006 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:39,007 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:40,008 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:41,009 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:42,010 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:43,011 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:44,011 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:45,012 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:46,013 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:46,015 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 7 failover attempts. Trying to failover immediately.
2019-09-28 06:00:47,016 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:47,028 [ReplicationMonitor] INFO  container.ReplicationManager (ReplicationManager.java:run(225)) - Replication Monitor Thread took 0 milliseconds for processing 0 containers.
2019-09-28 06:00:48,016 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:49,018 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:50,018 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:51,019 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:52,020 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:53,021 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:54,022 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:55,023 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:56,024 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:56,025 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 8 failover attempts. Trying to failover immediately.
2019-09-28 06:00:57,026 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:58,027 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:00:59,028 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:00,029 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:01,030 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:02,031 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:03,032 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:04,033 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:05,034 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:06,034 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:06,035 [main] INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking $Proxy36.submitRequest over nodeId=null,nodeAddress=127.0.0.1:0 after 9 failover attempts. Trying to failover immediately.
2019-09-28 06:01:07,036 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:08,037 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:09,038 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:10,039 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:11,040 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:12,041 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:13,042 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:14,043 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:15,044 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:16,045 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-28 06:01:16,046 [main] ERROR ha.OMFailoverProxyProvider (OzoneManagerProtocolClientSideTranslatorPB.java:getRetryAction(268)) - Failed to connect to OM. Attempted 10 retries and 10 failovers
2019-09-28 06:01:16,046 [main] ERROR client.OzoneClientFactory (OzoneClientFactory.java:getClientProtocol(259)) - Couldn't create RpcClient protocol exception: 
java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort
	at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy36.submitRequest(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy36.submitRequest(Unknown Source)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:338)
	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1223)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)
	at com.sun.proxy.$Proxy37.getServiceInfo(Unknown Source)
	at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:154)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:256)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:239)
	at org.apache.hadoop.ozone.client.OzoneClientFactory.getClient(OzoneClientFactory.java:75)
	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.init(RandomKeyGenerator.java:242)
	at org.apache.hadoop.ozone.freon.RandomKeyGenerator.call(RandomKeyGenerator.java:262)
	at org.apache.hadoop.ozone.freon.TestDataValidate.validateWriteTest(TestDataValidate.java:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 55 more
2019-09-28 06:01:16,048 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:shutdown(314)) - Shutting down the Mini Ozone Cluster
2019-09-28 06:01:16,049 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stop(330)) - Stopping the Mini Ozone Cluster
2019-09-28 06:01:16,049 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopOM(400)) - Stopping the OzoneManager
2019-09-28 06:01:16,049 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 43113
2019-09-28 06:01:16,056 [IPC Server listener on 43113] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 43113
2019-09-28 06:01:16,056 [main] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:stop(248)) - Stopping OMDoubleBuffer flush thread
2019-09-28 06:01:16,058 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-28 06:01:16,062 [OMDoubleBufferFlushThread] INFO  ratis.OzoneManagerDoubleBuffer (OzoneManagerDoubleBuffer.java:flushTransactions(191)) - OMDoubleBuffer flush thread OMDoubleBufferFlushThread is interrupted and will exit. OMDoubleBufferFlushThread
2019-09-28 06:01:16,069 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service KeyDeletingService
2019-09-28 06:01:16,073 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@3e850122{/,null,UNAVAILABLE}{/ozoneManager}
2019-09-28 06:01:16,077 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@27fde870{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-28 06:01:16,078 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1cbb3d3b{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-ozone-ozone-manager/0.5.0-SNAPSHOT/hadoop-ozone-ozone-manager-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-28 06:01:16,078 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@22d1886d{/logs,file:///workdir/hadoop-ozone/tools/target/log,UNAVAILABLE}
2019-09-28 06:01:16,083 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopDatanodes(377)) - Stopping the HddsDatanodes
2019-09-28 06:01:16,351 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-28 06:01:16,357 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-28 06:01:21,086 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-28 06:01:21,086 [main] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-28 06:01:21,087 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: close
2019-09-28 06:01:21,087 [main] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - bc198d42-aabb-46be-826b-770a90b8899e: close
2019-09-28 06:01:21,090 [main] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7: shutdown
2019-09-28 06:01:21,090 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7: shutdown
2019-09-28 06:01:21,090 [main] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-157546BB1BA7,id=bc198d42-aabb-46be-826b-770a90b8899e
2019-09-28 06:01:21,091 [main] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - bc198d42-aabb-46be-826b-770a90b8899e: shutdown FollowerState
2019-09-28 06:01:21,091 [ForkJoinPool.commonPool-worker-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-157546BB1BA7,id=86ebeb77-e9e6-4e56-959d-17a96123c217
2019-09-28 06:01:21,091 [Thread-260] INFO  impl.FollowerState (FollowerState.java:run(115)) - bc198d42-aabb-46be-826b-770a90b8899e: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-28 06:01:21,091 [main] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7-StateMachineUpdater: set stopIndex = 0
2019-09-28 06:01:21,092 [ForkJoinPool.commonPool-worker-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: shutdown LeaderState
2019-09-28 06:01:21,095 [main] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7: closes. applyIndex: 0
2019-09-28 06:01:21,099 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$438/1985072588@2a29d6e0] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(143)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7->6f360f6f-30d2-485d-bdb1-20146522ae70-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2019-09-28 06:01:21,099 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$438/1985072588@69348dd3] WARN  server.GrpcLogAppender (GrpcLogAppender.java:mayWait(143)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7->bc198d42-aabb-46be-826b-770a90b8899e-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
2019-09-28 06:01:21,100 [ForkJoinPool.commonPool-worker-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 86ebeb77-e9e6-4e56-959d-17a96123c217-PendingRequests: sendNotLeaderResponses
2019-09-28 06:01:21,110 [bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-28 06:01:21,110 [ForkJoinPool.commonPool-worker-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7-StateMachineUpdater: set stopIndex = 0
2019-09-28 06:01:21,113 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7: closes. applyIndex: 0
2019-09-28 06:01:21,113 [grpc-default-executor-1] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(113)) - bc198d42-aabb-46be-826b-770a90b8899e: Completed APPEND_ENTRIES, lastRequest: 86ebeb77-e9e6-4e56-959d-17a96123c217->bc198d42-aabb-46be-826b-770a90b8899e#130-t2, previous=(t:2, i:0), leaderCommit=0, initializing? false, entries: <empty>
2019-09-28 06:01:21,113 [main] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - bc198d42-aabb-46be-826b-770a90b8899e@group-157546BB1BA7-SegmentedRaftLogWorker close()
2019-09-28 06:01:21,113 [grpc-default-executor-2] INFO  server.GrpcServerProtocolService (GrpcServerProtocolService.java:onCompleted(113)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: Completed APPEND_ENTRIES, lastRequest: 86ebeb77-e9e6-4e56-959d-17a96123c217->6f360f6f-30d2-485d-bdb1-20146522ae70#130-t2, previous=(t:2, i:0), leaderCommit=0, initializing? false, entries: <empty>
2019-09-28 06:01:21,115 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-28 06:01:21,119 [main] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3: shutdown
2019-09-28 06:01:21,119 [ForkJoinPool.commonPool-worker-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7-SegmentedRaftLogWorker close()
2019-09-28 06:01:21,120 [main] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-707AEF46CAE3,id=bc198d42-aabb-46be-826b-770a90b8899e
2019-09-28 06:01:21,123 [grpc-default-executor-2] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(293)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7->6f360f6f-30d2-485d-bdb1-20146522ae70-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2019-09-28 06:01:21,123 [grpc-default-executor-1] INFO  server.GrpcLogAppender (GrpcLogAppender.java:onCompleted(293)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7->bc198d42-aabb-46be-826b-770a90b8899e-AppendLogResponseHandler: follower responses appendEntries COMPLETED
2019-09-28 06:01:21,122 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4: shutdown
2019-09-28 06:01:21,123 [main] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - bc198d42-aabb-46be-826b-770a90b8899e: shutdown LeaderState
2019-09-28 06:01:21,126 [ForkJoinPool.commonPool-worker-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-DECF01526FF4,id=86ebeb77-e9e6-4e56-959d-17a96123c217
2019-09-28 06:01:21,129 [main] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - bc198d42-aabb-46be-826b-770a90b8899e-PendingRequests: sendNotLeaderResponses
2019-09-28 06:01:21,130 [grpc-default-executor-1] INFO  impl.FollowerInfo (FollowerInfo.java:lambda$new$0(51)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7->bc198d42-aabb-46be-826b-770a90b8899e: nextIndex: updateUnconditionally 1 -> 0
2019-09-28 06:01:21,130 [ForkJoinPool.commonPool-worker-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: shutdown LeaderState
2019-09-28 06:01:21,130 [grpc-default-executor-2] INFO  impl.FollowerInfo (FollowerInfo.java:lambda$new$0(51)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-157546BB1BA7->6f360f6f-30d2-485d-bdb1-20146522ae70: nextIndex: updateUnconditionally 1 -> 0
2019-09-28 06:01:21,133 [ForkJoinPool.commonPool-worker-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 86ebeb77-e9e6-4e56-959d-17a96123c217-PendingRequests: sendNotLeaderResponses
2019-09-28 06:01:21,132 [main] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3-StateMachineUpdater: set stopIndex = 0
2019-09-28 06:01:21,133 [ForkJoinPool.commonPool-worker-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4-StateMachineUpdater: set stopIndex = 0
2019-09-28 06:01:21,133 [main] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3: closes. applyIndex: 0
2019-09-28 06:01:21,134 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4: closes. applyIndex: 0
2019-09-28 06:01:21,134 [bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-28 06:01:21,134 [86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-28 06:01:21,136 [main] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - bc198d42-aabb-46be-826b-770a90b8899e@group-707AEF46CAE3-SegmentedRaftLogWorker close()
2019-09-28 06:01:21,137 [ForkJoinPool.commonPool-worker-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 86ebeb77-e9e6-4e56-959d-17a96123c217@group-DECF01526FF4-SegmentedRaftLogWorker close()
2019-09-28 06:01:21,141 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - bc198d42-aabb-46be-826b-770a90b8899e: shutdown server with port 37666 now
2019-09-28 06:01:21,141 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: shutdown server with port 44635 now
2019-09-28 06:01:21,148 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - 86ebeb77-e9e6-4e56-959d-17a96123c217: shutdown server with port 44635 successfully
2019-09-28 06:01:21,148 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - bc198d42-aabb-46be-826b-770a90b8899e: shutdown server with port 37666 successfully
2019-09-28 06:01:21,153 [refreshUsed-/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-1/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-28 06:01:21,153 [refreshUsed-/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-2/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-28 06:01:21,181 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-28 06:01:21,182 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-28 06:01:21,186 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-28 06:01:21,191 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@77b919a3{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-28 06:01:21,192 [main] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-28 06:01:21,193 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@5624657a{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-28 06:01:21,194 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@30b131b2{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-28 06:01:21,194 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@378cfecf{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-28 06:01:21,195 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@4da6d664{/logs,file:///workdir/hadoop-ozone/tools/target/log,UNAVAILABLE}
2019-09-28 06:01:21,196 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@97d0c06{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-28 06:01:21,196 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@118ffcfd{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-28 06:01:21,196 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5a6fa56e{/logs,file:///workdir/hadoop-ozone/tools/target/log,UNAVAILABLE}
2019-09-28 06:01:21,445 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-28 06:01:21,994 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-28 06:01:26,196 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-28 06:01:26,197 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8: close
2019-09-28 06:01:26,197 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE: shutdown
2019-09-28 06:01:26,197 [ForkJoinPool.commonPool-worker-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-E59B61FA56EE,id=1db3c405-2e55-4b17-b9d1-8ec21761eee8
2019-09-28 06:01:26,198 [ForkJoinPool.commonPool-worker-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8: shutdown LeaderState
2019-09-28 06:01:26,198 [ForkJoinPool.commonPool-worker-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8-PendingRequests: sendNotLeaderResponses
2019-09-28 06:01:26,198 [main] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-28 06:01:26,198 [ForkJoinPool.commonPool-worker-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE-StateMachineUpdater: set stopIndex = 0
2019-09-28 06:01:26,199 [main] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - 301de80a-213b-4abd-bdb2-9b127524bed9: close
2019-09-28 06:01:26,199 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE: closes. applyIndex: 0
2019-09-28 06:01:26,199 [main] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0: shutdown
2019-09-28 06:01:26,199 [1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-28 06:01:26,200 [main] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-51BD14811DA0,id=301de80a-213b-4abd-bdb2-9b127524bed9
2019-09-28 06:01:26,201 [ForkJoinPool.commonPool-worker-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8@group-E59B61FA56EE-SegmentedRaftLogWorker close()
2019-09-28 06:01:26,201 [main] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 301de80a-213b-4abd-bdb2-9b127524bed9: shutdown LeaderState
2019-09-28 06:01:26,203 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8: shutdown server with port 33114 now
2019-09-28 06:01:26,203 [main] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 301de80a-213b-4abd-bdb2-9b127524bed9-PendingRequests: sendNotLeaderResponses
2019-09-28 06:01:26,204 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - 1db3c405-2e55-4b17-b9d1-8ec21761eee8: shutdown server with port 33114 successfully
2019-09-28 06:01:26,204 [main] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0-StateMachineUpdater: set stopIndex = 0
2019-09-28 06:01:26,205 [main] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0: closes. applyIndex: 0
2019-09-28 06:01:26,205 [refreshUsed-/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-0/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-28 06:01:26,205 [301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-28 06:01:26,207 [main] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 301de80a-213b-4abd-bdb2-9b127524bed9@group-51BD14811DA0-SegmentedRaftLogWorker close()
2019-09-28 06:01:26,208 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - 301de80a-213b-4abd-bdb2-9b127524bed9: shutdown server with port 38999 now
2019-09-28 06:01:26,210 [main] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - 301de80a-213b-4abd-bdb2-9b127524bed9: shutdown server with port 38999 successfully
2019-09-28 06:01:26,211 [refreshUsed-/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-4/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-28 06:01:26,226 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-28 06:01:26,229 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-28 06:01:26,230 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-28 06:01:26,231 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@625d9132{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-28 06:01:26,234 [main] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-28 06:01:26,234 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@77774571{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-28 06:01:26,235 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@80bfdc6{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-28 06:01:26,236 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@895416d{/logs,file:///workdir/hadoop-ozone/tools/target/log,UNAVAILABLE}
2019-09-28 06:01:26,236 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@7e53339{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-28 06:01:26,237 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@58fef7f7{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-28 06:01:26,237 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@76ad6715{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-28 06:01:26,238 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5ea4300e{/logs,file:///workdir/hadoop-ozone/tools/target/log,UNAVAILABLE}
2019-09-28 06:01:26,361 [Datanode State Machine Thread - 0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:lambda$startDaemon$0(350)) - Ozone container server started.
2019-09-28 06:01:27,599 [Thread-261] INFO  impl.FollowerState (FollowerState.java:run(106)) - 6f360f6f-30d2-485d-bdb1-20146522ae70:group-157546BB1BA7 changes to CANDIDATE, lastRpcTime:8808, electionTimeout:5075ms
2019-09-28 06:01:27,599 [Thread-261] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: shutdown FollowerState
2019-09-28 06:01:27,599 [Thread-261] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
2019-09-28 06:01:27,599 [Thread-261] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: start LeaderElection
2019-09-28 06:01:27,600 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10] INFO  impl.RaftServerImpl (ServerState.java:setLeader(253)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7: change Leader from 86ebeb77-e9e6-4e56-959d-17a96123c217 to null at term 2 for initElection
2019-09-28 06:01:27,613 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10] INFO  impl.LeaderElection (LeaderElection.java:askForVotes(182)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10: begin an election at term 3 for 0: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 06:01:27,626 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10] INFO  impl.LeaderElection (LogUtils.java:infoOrTrace(149)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10 got exception when requesting votes: {}
java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.ratis.server.impl.LeaderElection.waitForResults(LeaderElection.java:263)
	at org.apache.ratis.server.impl.LeaderElection.askForVotes(LeaderElection.java:201)
	at org.apache.ratis.server.impl.LeaderElection.run(LeaderElection.java:133)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:233)
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:214)
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:139)
	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$RaftServerProtocolServiceBlockingStub.requestVote(RaftServerProtocolServiceGrpc.java:265)
	at org.apache.ratis.grpc.server.GrpcServerProtocolClient.requestVote(GrpcServerProtocolClient.java:90)
	at org.apache.ratis.grpc.server.GrpcService.requestVote(GrpcService.java:204)
	at org.apache.ratis.server.impl.LeaderElection.lambda$submitRequests$1(LeaderElection.java:238)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /192.168.151.82:37666
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
	at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
	at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
Caused by: java.net.ConnectException: Connection refused
	... 11 more
2019-09-28 06:01:27,627 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10] INFO  impl.LeaderElection (LogUtils.java:infoOrTrace(149)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10 got exception when requesting votes: {}
java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.ratis.server.impl.LeaderElection.waitForResults(LeaderElection.java:263)
	at org.apache.ratis.server.impl.LeaderElection.askForVotes(LeaderElection.java:201)
	at org.apache.ratis.server.impl.LeaderElection.run(LeaderElection.java:133)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:233)
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:214)
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:139)
	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$RaftServerProtocolServiceBlockingStub.requestVote(RaftServerProtocolServiceGrpc.java:265)
	at org.apache.ratis.grpc.server.GrpcServerProtocolClient.requestVote(GrpcServerProtocolClient.java:90)
	at org.apache.ratis.grpc.server.GrpcService.requestVote(GrpcService.java:204)
	at org.apache.ratis.server.impl.LeaderElection.lambda$submitRequests$1(LeaderElection.java:238)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /192.168.151.82:44635
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
	at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
	at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
Caused by: java.net.ConnectException: Connection refused
	... 11 more
2019-09-28 06:01:27,628 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10] INFO  impl.LeaderElection (LeaderElection.java:logAndReturn(56)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10: Election REJECTED; received 0 response(s) [] and 2 exception(s); 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:t3, leader=null, voted=6f360f6f-30d2-485d-bdb1-20146522ae70, raftlog=6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [bc198d42-aabb-46be-826b-770a90b8899e:192.168.151.82:37666, 6f360f6f-30d2-485d-bdb1-20146522ae70:192.168.151.82:38933, 86ebeb77-e9e6-4e56-959d-17a96123c217:192.168.151.82:44635], old=null
2019-09-28 06:01:27,629 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10] INFO  impl.LeaderElection (LogUtils.java:infoOrTrace(149)) -   Exception 0: {}
java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.ratis.server.impl.LeaderElection.waitForResults(LeaderElection.java:263)
	at org.apache.ratis.server.impl.LeaderElection.askForVotes(LeaderElection.java:201)
	at org.apache.ratis.server.impl.LeaderElection.run(LeaderElection.java:133)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:233)
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:214)
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:139)
	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$RaftServerProtocolServiceBlockingStub.requestVote(RaftServerProtocolServiceGrpc.java:265)
	at org.apache.ratis.grpc.server.GrpcServerProtocolClient.requestVote(GrpcServerProtocolClient.java:90)
	at org.apache.ratis.grpc.server.GrpcService.requestVote(GrpcService.java:204)
	at org.apache.ratis.server.impl.LeaderElection.lambda$submitRequests$1(LeaderElection.java:238)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /192.168.151.82:37666
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
	at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
	at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
Caused by: java.net.ConnectException: Connection refused
	... 11 more
2019-09-28 06:01:27,629 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10] INFO  impl.LeaderElection (LogUtils.java:infoOrTrace(149)) -   Exception 1: {}
java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.ratis.server.impl.LeaderElection.waitForResults(LeaderElection.java:263)
	at org.apache.ratis.server.impl.LeaderElection.askForVotes(LeaderElection.java:201)
	at org.apache.ratis.server.impl.LeaderElection.run(LeaderElection.java:133)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:233)
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:214)
	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:139)
	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$RaftServerProtocolServiceBlockingStub.requestVote(RaftServerProtocolServiceGrpc.java:265)
	at org.apache.ratis.grpc.server.GrpcServerProtocolClient.requestVote(GrpcServerProtocolClient.java:90)
	at org.apache.ratis.grpc.server.GrpcService.requestVote(GrpcService.java:204)
	at org.apache.ratis.server.impl.LeaderElection.lambda$submitRequests$1(LeaderElection.java:238)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /192.168.151.82:44635
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496)
	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458)
	at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)
	at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
Caused by: java.net.ConnectException: Connection refused
	... 11 more
2019-09-28 06:01:27,630 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10] INFO  impl.RaftServerImpl (RaftServerImpl.java:setRole(174)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7: changes role from CANDIDATE to FOLLOWER at term 3 for DISCOVERED_A_NEW_TERM
2019-09-28 06:01:27,630 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: shutdown LeaderElection
2019-09-28 06:01:27,630 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7:LeaderElection10] INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: start FollowerState
2019-09-28 06:01:31,237 [ForkJoinPool.commonPool-worker-0] INFO  ozoneimpl.OzoneContainer (OzoneContainer.java:stop(231)) - Attempting to stop container services.
2019-09-28 06:01:31,238 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(314)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: close
2019-09-28 06:01:31,239 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6: shutdown
2019-09-28 06:01:31,239 [ForkJoinPool.commonPool-worker-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-36418E16FEF6,id=6f360f6f-30d2-485d-bdb1-20146522ae70
2019-09-28 06:01:31,239 [ForkJoinPool.commonPool-worker-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderState(104)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: shutdown LeaderState
2019-09-28 06:01:31,240 [ForkJoinPool.commonPool-worker-0] INFO  impl.PendingRequests (PendingRequests.java:sendNotLeaderResponses(202)) - 6f360f6f-30d2-485d-bdb1-20146522ae70-PendingRequests: sendNotLeaderResponses
2019-09-28 06:01:31,240 [ForkJoinPool.commonPool-worker-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6-StateMachineUpdater: set stopIndex = 0
2019-09-28 06:01:31,241 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6: closes. applyIndex: 0
2019-09-28 06:01:31,241 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-28 06:01:31,242 [ForkJoinPool.commonPool-worker-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-36418E16FEF6-SegmentedRaftLogWorker close()
2019-09-28 06:01:31,243 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (RaftServerImpl.java:lambda$shutdown$3(251)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7: shutdown
2019-09-28 06:01:31,244 [ForkJoinPool.commonPool-worker-0] INFO  util.JmxRegister (JmxRegister.java:unregister(73)) - Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-157546BB1BA7,id=6f360f6f-30d2-485d-bdb1-20146522ae70
2019-09-28 06:01:31,244 [ForkJoinPool.commonPool-worker-0] INFO  impl.RoleInfo (RoleInfo.java:shutdownFollowerState(121)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: shutdown FollowerState
2019-09-28 06:01:31,244 [ForkJoinPool.commonPool-worker-0] INFO  impl.StateMachineUpdater (StateMachineUpdater.java:stopAndJoin(125)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7-StateMachineUpdater: set stopIndex = 0
2019-09-28 06:01:31,244 [Thread-590] INFO  impl.FollowerState (FollowerState.java:run(115)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
2019-09-28 06:01:31,245 [ForkJoinPool.commonPool-worker-0] INFO  impl.RaftServerImpl (ServerState.java:close(385)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7: closes. applyIndex: 0
2019-09-28 06:01:31,245 [6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7-SegmentedRaftLogWorker] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:run(313)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
2019-09-28 06:01:31,246 [ForkJoinPool.commonPool-worker-0] INFO  segmented.SegmentedRaftLogWorker (SegmentedRaftLogWorker.java:close(228)) - 6f360f6f-30d2-485d-bdb1-20146522ae70@group-157546BB1BA7-SegmentedRaftLogWorker close()
2019-09-28 06:01:31,247 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(164)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: shutdown server with port 38933 now
2019-09-28 06:01:31,250 [ForkJoinPool.commonPool-worker-0] INFO  server.GrpcService (GrpcService.java:closeImpl(172)) - 6f360f6f-30d2-485d-bdb1-20146522ae70: shutdown server with port 38933 successfully
2019-09-28 06:01:31,260 [refreshUsed-/workdir/hadoop-ozone/tools/target/test-dir/MiniOzoneClusterImpl-05ae76d7-2308-4677-a01f-33fe42f97068/datanode-3/data/containers] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2019-09-28 06:01:31,274 [ForkJoinPool.commonPool-worker-0] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service BlockDeletingService
2019-09-28 06:01:31,276 [ForkJoinPool.commonPool-worker-0] INFO  statemachine.DatanodeStateMachine (DatanodeStateMachine.java:stopDaemon(395)) - Ozone container server stopped.
2019-09-28 06:01:31,278 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@67e13bd0{/,null,UNAVAILABLE}{/hddsDatanode}
2019-09-28 06:01:31,279 [ForkJoinPool.commonPool-worker-0] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@50fb33a{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-28 06:01:31,279 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@350d3f4d{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-container-service/0.5.0-SNAPSHOT/hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-28 06:01:31,280 [ForkJoinPool.commonPool-worker-0] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7fb29ca9{/logs,file:///workdir/hadoop-ozone/tools/target/log,UNAVAILABLE}
2019-09-28 06:01:31,281 [main] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:stopSCM(392)) - Stopping the StorageContainerManager
2019-09-28 06:01:31,281 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(808)) - Stopping Replication Manager Service.
2019-09-28 06:01:31,281 [main] INFO  container.ReplicationManager (ReplicationManager.java:stop(203)) - Stopping Replication Monitor Thread.
2019-09-28 06:01:31,281 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(815)) - Stopping Lease Manager of the command watchers
2019-09-28 06:01:31,282 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(822)) - Stopping datanode service RPC server
2019-09-28 06:01:31,282 [main] INFO  server.SCMDatanodeProtocolServer (SCMDatanodeProtocolServer.java:stop(367)) - Stopping the RPC server for DataNodes
2019-09-28 06:01:31,282 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 40752
2019-09-28 06:01:31,285 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-28 06:01:31,285 [IPC Server listener on 40752] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 40752
2019-09-28 06:01:31,331 [SCM Heartbeat Processing Thread - 0] WARN  node.NodeStateManager (NodeStateManager.java:scheduleNextHealthCheck(646)) - Current Thread is interrupted, shutting down HB processing thread for Node Manager.
2019-09-28 06:01:31,332 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(830)) - Stopping block service RPC server
2019-09-28 06:01:31,332 [main] INFO  server.SCMBlockProtocolServer (SCMBlockProtocolServer.java:stop(155)) - Stopping the RPC server for Block Protocol
2019-09-28 06:01:31,333 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 41337
2019-09-28 06:01:31,335 [IPC Server listener on 41337] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 41337
2019-09-28 06:01:31,335 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(837)) - Stopping the StorageContainerLocationProtocol RPC server
2019-09-28 06:01:31,335 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-28 06:01:31,335 [main] INFO  server.SCMClientProtocolServer (SCMClientProtocolServer.java:stop(158)) - Stopping the RPC server for Client Protocol
2019-09-28 06:01:31,335 [main] INFO  ipc.Server (Server.java:stop(3082)) - Stopping server on 37564
2019-09-28 06:01:31,336 [IPC Server listener on 37564] INFO  ipc.Server (Server.java:run(1185)) - Stopping IPC Server listener on 37564
2019-09-28 06:01:31,336 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(844)) - Stopping Storage Container Manager HTTP server.
2019-09-28 06:01:31,337 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1319)) - Stopping IPC Server Responder
2019-09-28 06:01:31,338 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@544630b7{/,null,UNAVAILABLE}{/scm}
2019-09-28 06:01:31,339 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@514eedd8{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
2019-09-28 06:01:31,339 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@44040454{/static,jar:file:/home/user/.m2/repository/org/apache/hadoop/hadoop-hdds-server-scm/0.5.0-SNAPSHOT/hadoop-hdds-server-scm-0.5.0-SNAPSHOT.jar!/webapps/static,UNAVAILABLE}
2019-09-28 06:01:31,340 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@44e3a2b2{/logs,file:///workdir/hadoop-ozone/tools/target/log,UNAVAILABLE}
2019-09-28 06:01:31,341 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(855)) - Stopping Block Manager Service.
2019-09-28 06:01:31,341 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service SCMBlockDeletingService
2019-09-28 06:01:31,342 [main] INFO  utils.BackgroundService (BackgroundService.java:shutdown(148)) - Shutting down service SCMBlockDeletingService
2019-09-28 06:01:31,342 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:stop(877)) - Stopping SCM Event Queue.
2019-09-28 06:01:31,348 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping ratis metrics system...
2019-09-28 06:01:31,353 [prometheus] INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:publishMetricsFromQueue(141)) - prometheus thread interrupted.
2019-09-28 06:01:31,353 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - ratis metrics system stopped.
